{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a seed for reproduceability\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer, dataset, and embedding model\n",
    "\n",
    "In this notebook, we review how to import a tokenizer, dataset and create our simplest model to train: an embedding model.  \n",
    "\n",
    "### Loading the Cosmo2 tokenizer\n",
    "\n",
    "Let's begin with loading a tokenizer.  The defualt LLama2 tokenizer is perfectly fine, but the nice people at Huggingface have a more up-to-date tokenizer, HuggingFaceTB/cosmo2-tokenizer, they use for the SmolLM models, which like our repo is based off of the LLama framework. We will use use the same. \n",
    "\n",
    "If this does not make sense what we are doing, please watch Andrej Kaparty's videos described in the Readme. We will leave it to his videos to explain what a tokenizer is, and how you might code one yourself.  We will just start from the beginning with a modern tokenizer used in the real world. \n",
    "\n",
    "The default context length for this tokenizer is 2048.  We will change that to 512 because we are training a simpler model.  Why 512?  For one, that was the orignal context length used in the GPT model.  Second, because the average paragraph has ~200 words, and there are roughly 2 tokens per word, so 512 ensures our model can at least understand paragraphs.  We will discuss how to increase the context length after training the initial models later like they do in the Llama 3 paper. \n",
    "\n",
    "See [the SmolLM settings](https://github.com/huggingface/smollm/blob/main/pre-training/smollm2/config_smollm2_135M.yaml) for more info on the SmolLM tokenizer, dataset, and other model choices if you want to explore further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 512 is a good minimal context length in principle large enough to understand paragraphs\n",
    "context_length = 512\n",
    "\n",
    "# The original Llama 2 tokenizer is available from Huggingface, but we will use the more\n",
    "# up-to-date Cosmo2 toenizer provided by Huggingface for the Llama-like SmolLM models\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/cosmo2-tokenizer\")\n",
    "tokenizer.model_max_length = context_length\n",
    "# There are times we need to pad.  We will make the padding token the same one that \n",
    "# signals end of sentance.\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Cosmo2 dataset\n",
    "\n",
    "We do not have access to the exact dataset used by Llama, some of which may be proprietary to Meta. We will use the Cosmo2 dataset provided by Huggingface used to train their LLama-like SmolLM models.  This dataset is quite large so we will use the streaming option to not have to download the whole thing.  If you must download, try to download a small portion of use a smaller dataset available on Huggingface like OpenWebtext. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ba14b8751e4ac69f85ceb9f90f8828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69444cf00cf405e99d977d1dfa37677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['prompt', 'text', 'token_length', 'audience', 'format', 'seed_data'],\n",
      "    num_shards: 104\n",
      "})\n",
      "\n",
      "Here is the text from the first entry:\n",
      "\n",
      " In today's ever-evolving world, technology has become an integral part of our lives, shaping the way we learn, work, and communicate. The COVID-19 pandemic has only accelerated this trend, forcing educational institutions worldwide to adapt quickly to remote learning models. As such, social studies integration in elementary education can greatly benefit from incorporating digital tools like those offered by Ruangguru's Online School. Let's explore how educators can effectively leverage these resources to create engaging and meaningful learning experiences for young students.\n",
      "\n",
      "Firstly, let's define what we mean by social studies integration. Social studies encompasses various disciplines that help students understand their communities, societies, and the wider world around them. These subjects may include history, geography, civics, economics, sociology, and anthropology. By integrating social studies into the curriculum, we aim to foster critical thinking skills, promote cultural awareness, and encourage responsible citizenship among students. Now, let's see how Ruangguru's Online School supports these goals.\n",
      "\n",
      "Ruangguru's Online School offers a wealth of opportunities for elementary students to engage with social studies topics in a dynamic and interactive manner. For instance, instead of simply reading about historical events or figures, students can participate in virtual simulations where they assume roles within specific time periods or cultures. Such immersive experiences allow children to connect emotionally with the material while developing essential collaboration and problem-solving abilities.\n",
      "\n",
      "Another significant advantage of utilizing Ruangguru's platform is accessibility. With many schools closed due to the ongoing health crisis, ensuring continuity of learning has become paramount. Through Ruangguru's mobile application, students can access quality instructional materials anytime, anywhere—an especially crucial feature for families without reliable internet connections at home. Moreover, the platform allows teachers to track student progress, enabling them to provide targeted support and intervention when necessary.\n",
      "\n",
      "Furthermore, Ruangguru recognizes the importance of professional development for educators transitioning to remote teaching environments. To facilitate this process, they offer complimentary online teacher training sessions via their app. During these workshops, instructors can share best practices, discuss challenges, and collaborate on innovative strategies for delivering effective social studies lessons in a virtual setting. Additionally, connecting with fellow professionals fosters a sense of community among teachers, helping them feel supported and valued during uncertain times.\n",
      "\n",
      "Moreover, Ruangguru's Online School aligns seamlessly with current trends emphasizing project-based learning (PBL), which encourages students to apply knowledge and skills to solve authentic problems. For example, students might research local environmental issues, analyze data, and propose solutions using multimedia presentations created within the app. PBL approaches not only deepen conceptual understanding but also empower students to take action and contribute positively to their communities.\n",
      "\n",
      "Lastly, Ruangguru's commitment to providing equitable access to quality education resonates strongly with recent calls for social justice reform in education. By offering free resources and training, they democratize learning opportunities, allowing all students to thrive regardless of socioeconomic background. Furthermore, incorporating diverse perspectives and histories into social studies curricula promotes inclusivity and empathy, preparing future generations to navigate an increasingly interconnected global society.\n",
      "\n",
      "In conclusion, harnessing the power of digital platforms like Ruangguru's Online School can significantly enhance social studies integration efforts at the elementary level. Amidst unprecedented challenges brought forth by the COVID-19 pandemic, embracing innovation becomes vital in creating resilient learners who are well-equipped to tackle complex issues facing our world today. Together, let us strive towards cultivating engaged, informed, and compassionate citizens ready to make a difference in their communities and beyond.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from hugging face in streaming mode.\n",
    "dataset = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True)\n",
    "\n",
    "# The dataset is an IterableDataset. The 'text' field is the feature we want.\n",
    "print (dataset)\n",
    "\n",
    "# Let's extract and look at the text from the first entry\n",
    "item = next(iter(dataset))['text']\n",
    "print ('\\nHere is the text from the first entry:\\n')\n",
    "print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  533,  1834,   506,  2042,    29, 24186,   905,    28,  1835,   553,\n",
      "          1438,   354, 10021,   599,   282,   653,  2397,    28,  5929,   260,\n",
      "           970,   392,   835,    28,   746,    28,   284,  5865,    30,   378,\n",
      "          7191,    29,    33,    41,  8832,   553,   805, 18438,   451,  4213,\n",
      "            28, 14072,  4096,  4679,  4969,   288,  2930,  3112,   288,  6431,\n",
      "          1380,  2859,    30,  1032,   715,    28,  1329,  2157,  7657,   281,\n",
      "         11289,  1888,   416,  6899,  3724,   429,  6457,  3182,  2549,   702,\n",
      "           967,  5340,   411, 20139,   604,    87, 16634,   506,  8646,  3056,\n",
      "            30,  2959,   506,  2217,   638,  7537,   416,  4025, 17188,   623,\n",
      "          1952,   288,  1464,  4798,   284,  6579,  1380,  2647,   327,  1805,\n",
      "          1058,    30,   198,   198, 21028,    28,  1303,   506,  5879,   732,\n",
      "           392,  1441,   411,  1329,  2157,  7657,    30,  4758,  2157, 15868,\n",
      "          1461, 10578,   338,   724,  1058,  1044,   480,  2429,    28,  6815,\n",
      "            28,   284,   260,  9376,   905,  1130,   601,    30,  1216,  5219,\n",
      "           654,  1453,  1463,    28, 12973,    28, 14422,   747,    28,  9883,\n",
      "            28, 19776,    28,   284, 21840,    30,  1428, 10551,  1329,  2157,\n",
      "           618,   260,  6194,    28,   392,  2748,   288,  6366,  2609,  3039,\n",
      "          1954,    28,  4058,  2642,  4425,    28,   284,  4517,  3358, 14135,\n",
      "          1486,  1058,    30,  3569,    28,  1303,   506,   963,   638, 20139,\n",
      "           604,    87, 16634,   506,  8646,  3056,  6569,   623,  3949,    30,\n",
      "           198,   198,    66, 36112,    87, 16634,   506,  8646,  3056,  3362,\n",
      "           253,  5092,   282,  3036,   327, 11289,  1058,   288,  4555,   351,\n",
      "          1329,  2157,  4366,   281,   253,  6639,   284,  8446,  5710,    30,\n",
      "          1068,  2648,    28,  2489,   282,  2788,  2539,   563,  2844,  2466,\n",
      "           355,  5090,    28,  1058,   416,  6390,   281,  5798, 13621,   837,\n",
      "           502,  7635,  5191,  1127,  1678,   655,  5324,   355,  4520,    30,\n",
      "          5475, 18061,  2647,  1167,  1122,   288,  2084, 13937,   351,   260,\n",
      "          1376,   979,  2941,  1895,  5339,   284,  1732,    29, 11850,  6236,\n",
      "            30,   198,   198,  4809,  1546,  5163,   282, 11824, 20139,   604,\n",
      "            87, 16634,   506,  3941,   314, 11770,    30,  1929,   800,  2764,\n",
      "          6370,  1568,   288,   260,  5185,   864,  6051,    28,  4659, 16186,\n",
      "           282,  1380,   553,  1438, 15133,    30,  3121, 20139,   604,    87,\n",
      "         16634,   506,  6220,  3279,    28,  1058,   416,  1594,  2174, 14883,\n",
      "          2254, 22367,    28,  8471,  1265,   276,  2117,  3202,  3865,   327,\n",
      "          3168,  1355,  7002,  5594,  3962,   418,  1478,    30,  4790,    28,\n",
      "           260,  3941,  2518,  3176,   288,  4320,  2222,  2894,    28,  7426,\n",
      "           601,   288,  1538,  8291,  1199,   284,  5987,   645,  2371,    30,\n",
      "           198,   198, 16707,    28, 20139,   604,    87, 16634, 17084,   260,\n",
      "          2979,   282,  3544,  1421,   327,  7537, 29048,   288,  6431,  3229,\n",
      "          4978,    30,  1626,  7304,   451,   980,    28,   502,  2626, 48711,\n",
      "          2329,  3743,  2534,  7838,  3560,   480,   566,    30,  3897,   623,\n",
      "         10971,    28, 17231,   416,  2419,  1450,  2718,    28,  1692,  2529,\n",
      "            28,   284, 13326,   335,  5635,  3068,   327, 12457,  2430,  1329,\n",
      "          2157,  4431,   281,   253,  5798,  4054,    30,  4454,    28,  8377,\n",
      "           351,  6955,  4991, 12751,   253,  2588,   282,  1773,  1486,  3176,\n",
      "            28,  4307,   601,  1407,  5302,   284, 12637,   981,  7024,  1711,\n",
      "            30,   198,   198, 11725,    28, 20139,   604,    87, 16634,   506,\n",
      "          8646,  3056, 24578, 19218,   351,  1574,  6165, 12242,  1391,    29,\n",
      "          2542,  1380,   365,    64, 11471,   643,   527,  7420,  1058,   288,\n",
      "          3777,  1967,   284,  1954,   288,  5482,  8714,  1828,    30,  1068,\n",
      "          1183,    28,  1058,  1124,  1151,  1679,  2546,  1974,    28,  6524,\n",
      "           940,    28]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[  533,  1834,   506,  2042,    29, 24186,   905,    28,  1835,   553,\n",
      "          1438,   354, 10021,   599,   282,   653,  2397,    28,  5929,   260,\n",
      "           970,   392,   835,    28,   746,    28,   284,  5865,    30,   378,\n",
      "          7191,    29,    33,    41,  8832,   553,   805, 18438,   451,  4213,\n",
      "            28, 14072,  4096,  4679,  4969,   288,  2930,  3112,   288,  6431,\n",
      "          1380,  2859,    30,  1032,   715,    28,  1329,  2157,  7657,   281,\n",
      "         11289,  1888,   416,  6899,  3724,   429,  6457,  3182,  2549,   702,\n",
      "           967,  5340,   411, 20139,   604,    87, 16634,   506,  8646,  3056,\n",
      "            30,  2959,   506,  2217,   638,  7537,   416,  4025, 17188,   623,\n",
      "          1952,   288,  1464,  4798,   284,  6579,  1380,  2647,   327,  1805,\n",
      "          1058,    30,   198,   198, 21028,    28,  1303,   506,  5879,   732,\n",
      "           392,  1441,   411,  1329,  2157,  7657,    30,  4758,  2157, 15868,\n",
      "          1461, 10578,   338,   724,  1058,  1044,   480,  2429,    28,  6815,\n",
      "            28,   284,   260,  9376,   905,  1130,   601,    30,  1216,  5219,\n",
      "           654,  1453,  1463,    28, 12973,    28, 14422,   747,    28,  9883,\n",
      "            28, 19776,    28,   284, 21840,    30,  1428, 10551,  1329,  2157,\n",
      "           618,   260,  6194,    28,   392,  2748,   288,  6366,  2609,  3039,\n",
      "          1954,    28,  4058,  2642,  4425,    28,   284,  4517,  3358, 14135,\n",
      "          1486,  1058,    30,  3569,    28,  1303,   506,   963,   638, 20139,\n",
      "           604,    87, 16634,   506,  8646,  3056,  6569,   623,  3949,    30,\n",
      "           198,   198,    66, 36112,    87, 16634,   506,  8646,  3056,  3362,\n",
      "           253,  5092,   282,  3036,   327, 11289,  1058,   288,  4555,   351,\n",
      "          1329,  2157,  4366,   281,   253,  6639,   284,  8446,  5710,    30,\n",
      "          1068,  2648,    28,  2489,   282,  2788,  2539,   563,  2844,  2466,\n",
      "           355,  5090,    28,  1058,   416,  6390,   281,  5798, 13621,   837,\n",
      "           502,  7635,  5191,  1127,  1678,   655,  5324,   355,  4520,    30,\n",
      "          5475, 18061,  2647,  1167,  1122,   288,  2084, 13937,   351,   260,\n",
      "          1376,   979,  2941,  1895,  5339,   284,  1732,    29, 11850,  6236,\n",
      "            30,   198,   198,  4809,  1546,  5163,   282, 11824, 20139,   604,\n",
      "            87, 16634,   506,  3941,   314, 11770,    30,  1929,   800,  2764,\n",
      "          6370,  1568,   288,   260,  5185,   864,  6051,    28,  4659, 16186,\n",
      "           282,  1380,   553,  1438, 15133,    30,  3121, 20139,   604,    87,\n",
      "         16634,   506,  6220,  3279,    28,  1058,   416,  1594,  2174, 14883,\n",
      "          2254, 22367,    28,  8471,  1265,   276,  2117,  3202,  3865,   327,\n",
      "          3168,  1355,  7002,  5594,  3962,   418,  1478,    30,  4790,    28,\n",
      "           260,  3941,  2518,  3176,   288,  4320,  2222,  2894,    28,  7426,\n",
      "           601,   288,  1538,  8291,  1199,   284,  5987,   645,  2371,    30,\n",
      "           198,   198, 16707,    28, 20139,   604,    87, 16634, 17084,   260,\n",
      "          2979,   282,  3544,  1421,   327,  7537, 29048,   288,  6431,  3229,\n",
      "          4978,    30,  1626,  7304,   451,   980,    28,   502,  2626, 48711,\n",
      "          2329,  3743,  2534,  7838,  3560,   480,   566,    30,  3897,   623,\n",
      "         10971,    28, 17231,   416,  2419,  1450,  2718,    28,  1692,  2529,\n",
      "            28,   284, 13326,   335,  5635,  3068,   327, 12457,  2430,  1329,\n",
      "          2157,  4431,   281,   253,  5798,  4054,    30,  4454,    28,  8377,\n",
      "           351,  6955,  4991, 12751,   253,  2588,   282,  1773,  1486,  3176,\n",
      "            28,  4307,   601,  1407,  5302,   284, 12637,   981,  7024,  1711,\n",
      "            30,   198,   198, 11725,    28, 20139,   604,    87, 16634,   506,\n",
      "          8646,  3056, 24578, 19218,   351,  1574,  6165, 12242,  1391,    29,\n",
      "          2542,  1380,   365,    64, 11471,   643,   527,  7420,  1058,   288,\n",
      "          3777,  1967,   284,  1954,   288,  5482,  8714,  1828,    30,  1068,\n",
      "          1183,    28,  1058,  1124,  1151,  1679,  2546,  1974,    28,  6524,\n",
      "           940,    28]])\n"
     ]
    }
   ],
   "source": [
    "# Let's tokenize that text.\n",
    "# The tokenizer by default \n",
    "x = tokenizer(item, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# As you can see the tokenizer has the tokens under 'input_ids'. It also creates \n",
    "# an attention mask to track which tokens to pay attention to.  IE, Not padding. \n",
    "print (x)\n",
    "\n",
    "# Now let's look at the input_ids\n",
    "print (x['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In today's ever-evolving world, technology has become an integral part of our lives, shaping the way we learn, work, and communicate. The COVID-19 pandemic has only accelerated this trend, forcing educational institutions worldwide to adapt quickly to remote learning models. As such, social studies integration in elementary education can greatly benefit from incorporating digital tools like those offered by Ruangguru's Online School. Let's explore how educators can effectively leverage these resources to create engaging and meaningful learning experiences for young students.\n",
      "\n",
      "Firstly, let's define what we mean by social studies integration. Social studies encompasses various disciplines that help students understand their communities, societies, and the wider world around them. These subjects may include history, geography, civics, economics, sociology, and anthropology. By integrating social studies into the curriculum, we aim to foster critical thinking skills, promote cultural awareness, and encourage responsible citizenship among students. Now, let's see how Ruangguru's Online School supports these goals.\n",
      "\n",
      "Ruangguru's Online School offers a wealth of opportunities for elementary students to engage with social studies topics in a dynamic and interactive manner. For instance, instead of simply reading about historical events or figures, students can participate in virtual simulations where they assume roles within specific time periods or cultures. Such immersive experiences allow children to connect emotionally with the material while developing essential collaboration and problem-solving abilities.\n",
      "\n",
      "Another significant advantage of utilizing Ruangguru's platform is accessibility. With many schools closed due to the ongoing health crisis, ensuring continuity of learning has become paramount. Through Ruangguru's mobile application, students can access quality instructional materials anytime, anywhere—an especially crucial feature for families without reliable internet connections at home. Moreover, the platform allows teachers to track student progress, enabling them to provide targeted support and intervention when necessary.\n",
      "\n",
      "Furthermore, Ruangguru recognizes the importance of professional development for educators transitioning to remote teaching environments. To facilitate this process, they offer complimentary online teacher training sessions via their app. During these workshops, instructors can share best practices, discuss challenges, and collaborate on innovative strategies for delivering effective social studies lessons in a virtual setting. Additionally, connecting with fellow professionals fosters a sense of community among teachers, helping them feel supported and valued during uncertain times.\n",
      "\n",
      "Moreover, Ruangguru's Online School aligns seamlessly with current trends emphasizing project-based learning (PBL), which encourages students to apply knowledge and skills to solve authentic problems. For example, students might research local environmental issues, analyze data,\n"
     ]
    }
   ],
   "source": [
    "# Let's check to make sure the tokenized text matechs the above\n",
    "print (tokenizer.decode(x['input_ids'][0]))\n",
    "\n",
    "# It appeads to check out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare tokenized dataset for training\n",
    "\n",
    "To train a model, we need the data to come out in batches of tokens.  To do this we need to map the dataset to tensors of shape (batch_size, context_length).\n",
    "\n",
    "We can do that as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# Create a tokenize fundtion.\n",
    "def tokenize(item):\n",
    "    x = tokenizer(\n",
    "        item['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset.column_names\n",
    ")\n",
    "train_dl = DataLoader(dataset=tokenized_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  533,  1834,   506,  ...,  6524,   940,    28],\n",
      "        [ 6682,    42,   198,  ...,    30, 17897,  5076],\n",
      "        [ 4281,    42, 21236,  ..., 32846,   618,   253],\n",
      "        ...,\n",
      "        [ 7606,    42, 24411,  ..., 24312,    28,   392],\n",
      "        [22419, 18452,    28,  ...,  1798,    28, 18452],\n",
      "        [ 4281,   216,    33,  ..., 33149,  2254,    28]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "# Let's text.  We should have an input_id field and attention_mask field \n",
    "# of shape (batch_size, context_length)\n",
    "x = next(iter(train_dl))\n",
    "\n",
    "print (x)\n",
    "print (x['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original entry 0 text is:  In today's ever-evolving world, technology has become an in\n",
      "Tokenized entry 0 text is:  In today's ever-evolving world, technology has become an integral part of\n",
      "\n",
      "Original entry 1 text is:  Introduction:\n",
      "In today's world, cooking has evolved beyond \n",
      "Tokenized entry 1 text is:  Introduction:\n",
      "In today's world, cooking has evolved beyond traditional methods such\n",
      "\n",
      "Original entry 2 text is:  Chapter: Cooking Up Comfort: Chemotheapy-Induced Nausea Rel\n",
      "Tokenized entry 2 text is:  Chapter: Cooking Up Comfort: Chemotheapy-Induced Nausea\n",
      "\n",
      "Original entry 3 text is:  Chapter 7: Navigating Orphanage Life - A Safe Haven or a Sp\n",
      "Tokenized entry 3 text is:  Chapter 7: Navigating Orphanage Life - A Safe Haven or a\n"
     ]
    }
   ],
   "source": [
    "# Once again, let's make sure we are getting back the right results. \n",
    "for i, item in enumerate(dataset):\n",
    "    print (f\"\\nOriginal entry {i} text is: {item['text'][:60]}\")\n",
    "    print (f\"Tokenized entry {i} text is: {tokenizer.decode(x['input_ids'][i][:15])}\")\n",
    "    if i > 2:\n",
    "        break\n",
    "\n",
    "# Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building embedding model\n",
    "\n",
    "Now that the dataset and tokenizer seem to be working, we need to construct a model. At first let's do the simplest thing: let's create a model that maps the tokens into an embedding space of dimension 576 to be consistant with SmolLM mentioned above.  Then let's map back.\n",
    "\n",
    "Also, we will try and keep these model sizes consistant at ~55M parameters.  This we we can see if improvement is coming from model size or if something about the archetecture is aiding learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will inherit from Pytorch's Module class\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, n_embed: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Map tokens to embedding space with dimension n_embed\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=n_embed)\n",
    "\n",
    "        # Now map back to make prediction for next tokens, called logits.  The layer that\n",
    "        # does this is traditionally a linear layer called the lannguage model head.\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.embedding.weight = self.lm_head.weight\n",
    "        \n",
    "\n",
    "    # Pytorch Modules have a forward method that is what is executed when the model is called\n",
    "    # The input is the (batch, tokens) tensor and a (batch, logits) tensor is returned\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.894464\n"
     ]
    }
   ],
   "source": [
    "# Let's create the model\n",
    "model = EmbeddingModel(vocab_size = tokenizer.vocab_size, n_embed=832)\n",
    "\n",
    "# Let's see how many parameters we have\n",
    "print (sum([p.numel() for p in model.parameters()])/1.0e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "\n",
    "Let's make the training loop as simple as possible.  Let's try to keep it as close to the pytorch quickstart example, only adding what is required for large language model traing: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "A few things that need to be added:\n",
    "\n",
    "* Some items are named to standard conventions like pred -> logits\n",
    "* LLama is trained with AdamW\n",
    "* The optimizer is Cross Entropy\n",
    "* Labels are the same as the input_ids, except we shift them so that we compare to the next token.\n",
    "\n",
    "If any of this does not make sense, again please watch Andrej's videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 40.89M  params\n",
      "The vocab size is 49152, meaning the initial loss should be ~10.803\n",
      "Step:     0,  Tokens:  0.008M,  Loss:  10.798, Perplexity:  48940.6\n",
      "Step:   500,  Tokens:  4.104M,  Loss:   9.070, Perplexity:   8690.6\n",
      "Step:  1000,  Tokens:  8.200M,  Loss:   7.893, Perplexity:   2677.8\n",
      "Step:  1500,  Tokens: 12.296M,  Loss:   7.911, Perplexity:   2727.2\n",
      "Step:  2000,  Tokens: 16.392M,  Loss:   7.670, Perplexity:   2142.8\n",
      "Step:  2500,  Tokens: 20.488M,  Loss:   7.526, Perplexity:   1856.2\n",
      "Step:  3000,  Tokens: 24.584M,  Loss:   7.749, Perplexity:   2318.9\n",
      "Step:  3500,  Tokens: 28.680M,  Loss:   7.015, Perplexity:   1113.7\n",
      "Step:  4000,  Tokens: 32.776M,  Loss:   7.379, Perplexity:   1601.2\n",
      "Step:  4500,  Tokens: 36.872M,  Loss:   7.344, Perplexity:   1547.3\n",
      "Step:  5000,  Tokens: 40.968M,  Loss:   7.290, Perplexity:   1465.9\n",
      "Step:  5500,  Tokens: 45.064M,  Loss:   7.226, Perplexity:   1374.1\n",
      "Step:  6000,  Tokens: 49.160M,  Loss:   7.504, Perplexity:   1815.0\n",
      "Step:  6500,  Tokens: 53.256M,  Loss:   7.173, Perplexity:   1304.4\n",
      "Step:  7000,  Tokens: 57.352M,  Loss:   7.082, Perplexity:   1190.7\n",
      "Step:  7500,  Tokens: 61.448M,  Loss:   6.934, Perplexity:   1026.2\n",
      "Step:  8000,  Tokens: 65.544M,  Loss:   6.907, Perplexity:    999.1\n",
      "Step:  8500,  Tokens: 69.640M,  Loss:   7.125, Perplexity:   1242.6\n",
      "Step:  9000,  Tokens: 73.736M,  Loss:   7.047, Perplexity:   1149.1\n",
      "Step:  9500,  Tokens: 77.832M,  Loss:   7.098, Perplexity:   1209.2\n",
      "Step: 10000,  Tokens: 81.928M,  Loss:   7.154, Perplexity:   1278.6\n",
      "Step: 10500,  Tokens: 86.024M,  Loss:   6.887, Perplexity:    979.0\n",
      "Step: 11000,  Tokens: 90.120M,  Loss:   7.077, Perplexity:   1183.9\n",
      "Step: 11500,  Tokens: 94.216M,  Loss:   6.749, Perplexity:    853.0\n",
      "Step: 12000,  Tokens: 98.312M,  Loss:   6.484, Perplexity:    654.5\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\"\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters()]) / 1.0e6\n",
    "print(f\"This model has {num_params:.2f}M  params\")\n",
    "print(\n",
    "    f\"The vocab size is {tokenizer.vocab_size}, meaning the initial loss should be ~{math.log(tokenizer.vocab_size):.3f}\"\n",
    ")\n",
    "model.train()\n",
    "losses = []\n",
    "for step, batch in enumerate(train_dl):\n",
    "\n",
    "    # Get data\n",
    "    x = batch[\"input_ids\"].to(device)\n",
    "    att_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Compute prediction error\n",
    "    logits = model(x)\n",
    "\n",
    "    # Shift to compare loss correctly\n",
    "    shifted_logits = logits[:, :-1].contiguous()\n",
    "    labels = x[:, 1:].contiguous()\n",
    "    loss = loss_fn(shifted_logits.view(-1, shifted_logits.size(-1)), labels.view(-1))\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(\n",
    "            f\"Step: {step:>5d},  Tokens:{batch_size*context_length*(step+1)/1.0e6:>7.3f}M,  Loss: {loss.item():>7.3f}, Perplexity:  {math.exp(loss.item()):>7.1f}\"\n",
    "        )\n",
    "    if (step + 1) % 12500 == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x169bf65d0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids for king is [644]\n",
      "Top 25 closest tokens to king by cosine similarity: \n",
      "Token: 42707,      deceptive,   Similarity: 0.1598\n",
      "Token:  2929,         custom,   Similarity: 0.1524\n",
      "Token: 23009,           Pred,   Similarity: 0.1452\n",
      "Token: 46574,           deem,   Similarity: 0.1403\n",
      "Token: 38763,           Snow,   Similarity: 0.1388\n",
      "Token: 44092,          juana,   Similarity: 0.1387\n",
      "Token: 37276,           THAT,   Similarity: 0.1367\n",
      "Token: 12123,            cod,   Similarity: 0.1309\n",
      "Token:  6414, ================,   Similarity: 0.1306\n",
      "Token: 37466,    indifferent,   Similarity: 0.1305\n",
      "Token: 40977,           avas,   Similarity: 0.1301\n",
      "Token: 11205,        undergo,   Similarity: 0.1297\n",
      "Token: 35989,          Genes,   Similarity: 0.1277\n",
      "Token:  9102,         breaks,   Similarity: 0.1272\n",
      "Token: 21110,      explosive,   Similarity: 0.1252\n",
      "Token: 36724,            Zam,   Similarity: 0.1222\n",
      "Token: 30420,          Neuro,   Similarity: 0.1221\n",
      "Token: 24233,          yards,   Similarity: 0.1221\n",
      "Token:  6349,        persons,   Similarity: 0.1220\n",
      "Token: 14096,      residence,   Similarity: 0.1214\n",
      "Token: 21444,          Roger,   Similarity: 0.1210\n",
      "Token: 48823,      shorthand,   Similarity: 0.1210\n",
      "Token: 36716,          asmod,   Similarity: 0.1200\n",
      "Token: 48254,          aimon,   Similarity: 0.1199\n",
      "Token: 47730,      superiors,   Similarity: 0.1198\n"
     ]
    }
   ],
   "source": [
    "# Assuming the embedding layer is already trained\n",
    "layer = model.embedding.to('cpu')\n",
    "\n",
    "# Get the embedding for 'king' (e.g., assuming 'king' corresponds to token index 0)\n",
    "k = 200\n",
    "word = 'king'\n",
    "\n",
    "print (f\"Input ids for {word} is {tokenizer(word)['input_ids']}\")\n",
    "\n",
    "word_index = tokenizer(word)['input_ids'][0]\n",
    "word_embedding = layer(torch.tensor(word_index)).detach()  # Get the embedding for 'king'\n",
    "\n",
    "# Get all embeddings\n",
    "all_embeddings = layer.weight.detach()  # Extract all embeddings\n",
    "\n",
    "\n",
    "# Compute cosine similarity between 'king' and all other tokens\n",
    "cosine_similarities = F.cosine_similarity(word_embedding.unsqueeze(0), all_embeddings, dim=1)\n",
    "\n",
    "# Get the indices of the 10 most similar tokens (excluding 'king' itself)\n",
    "top_k = torch.topk(cosine_similarities, k=k+1)  # Top 11 because 'king' will be in the list\n",
    "top_k_indices = top_k.indices[top_k.indices != word_index][:k]  # Exclude 'king'\n",
    "\n",
    "\n",
    "# Print the 10 closest tokens and their similarities\n",
    "print(f\"Top 25 closest tokens to {word} by cosine similarity: \")\n",
    "ii = 0\n",
    "for idx in top_k_indices:\n",
    "    #print(idx.item())\n",
    "    idx_word = tokenizer.decode(idx.item()).replace('\\n','')\n",
    "    if len(idx_word) > 3:\n",
    "        print(f\"Token: {idx.item():5d}, {idx_word:>14s},   Similarity: {cosine_similarities[idx].item():.4f}\")\n",
    "        ii += 1\n",
    "        if ii > 24:\n",
    "            break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 25 closest tokens to king by distance: \n",
      "Token: 42707,      deceptive,   Distance: 0.73\n",
      "Token: 37276,           THAT,   Distance: 0.74\n",
      "Token: 29174,     individual,   Distance: 0.75\n",
      "Token:  8424,     maintained,   Distance: 0.75\n",
      "Token: 36327,         clumps,   Distance: 0.75\n",
      "Token: 33130,           Levi,   Distance: 0.75\n",
      "Token: 18415,        realism,   Distance: 0.75\n",
      "Token: 11055,           cann,   Distance: 0.75\n",
      "Token: 33265,       authored,   Distance: 0.75\n",
      "Token: 18719,        pection,   Distance: 0.75\n",
      "Token: 11997,     ingredient,   Distance: 0.75\n",
      "Token:  2929,         custom,   Distance: 0.75\n",
      "Token: 35257,       software,   Distance: 0.75\n",
      "Token: 28942,       conducts,   Distance: 0.75\n",
      "Token: 11205,        undergo,   Distance: 0.75\n",
      "Token: 40529,        Factors,   Distance: 0.75\n",
      "Token: 21110,      explosive,   Distance: 0.75\n",
      "Token: 19412,      incentive,   Distance: 0.75\n",
      "Token: 38774,   Observations,   Distance: 0.75\n",
      "Token:  5860,         serves,   Distance: 0.75\n",
      "Token: 21444,          Roger,   Distance: 0.75\n",
      "Token: 26369,          later,   Distance: 0.75\n",
      "Token: 33498,    containment,   Distance: 0.75\n",
      "Token: 11289,     elementary,   Distance: 0.75\n",
      "Token:  6719,      alongside,   Distance: 0.75\n"
     ]
    }
   ],
   "source": [
    "distances = torch.norm(all_embeddings - word_embedding, dim=1)  # Compute L2 norm (Euclidean distance)\n",
    "\n",
    "# Get the indices of the 10 closest tokens (excluding 'king' itself)\n",
    "top_k = torch.topk(-distances, k=k+1)  # Use negative distances to mimic \\\"closest\\\"\n",
    "top_k_indices = top_k.indices[top_k.indices != word_index][:k]  # Exclude 'king'\n",
    "\n",
    "# Print the 10 closest tokens and their distances\n",
    "print(f\"\\nTop 25 closest tokens to {word} by distance: \")\n",
    "ii = 0\n",
    "for idx in top_k_indices:\n",
    "    #print(idx.item())\n",
    "    idx_word = tokenizer.decode(idx.item()).replace('\\n','')\n",
    "    if len(idx_word) > 4:    \n",
    "        print(f\"Token: {idx.item():5d}, {tokenizer.decode(idx.item()).replace('\\n',''):>14s},   Distance: {distances[idx].item():.2f}\")\n",
    "        ii += 1\n",
    "        if ii > 24:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.98336\n"
     ]
    }
   ],
   "source": [
    "# We will inherit from Pytorch's Module class\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, n_embed: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Map tokens to embedding space with dimension n_embed\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=n_embed)\n",
    "\n",
    "        self.fft1 = nn.Sequential(nn.Linear(n_embed, 4*n_embed),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(n_embed, 4*n_embed))\n",
    "        \n",
    "        self.fft2 = nn.Sequential(nn.Linear(n_embed, 4*n_embed),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(n_embed, 4*n_embed))\n",
    "                               \n",
    "\n",
    "        # Now map back to make prediction for next tokens, called logits.  The layer that\n",
    "        # does this is traditionally a linear layer called the lannguage model head.\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.embedding.weight = self.lm_head.weight\n",
    "        \n",
    "\n",
    "    # Pytorch Modules have a forward method that is what is executed when the model is called\n",
    "    # The input is the (batch, tokens) tensor and a (batch, logits) tensor is returned\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = self.fft1(x)\n",
    "        x = self.fft2(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "    \n",
    "# Let's create the model\n",
    "model = EmbeddingModel(vocab_size = tokenizer.vocab_size, n_embed=832)\n",
    "\n",
    "# Let's see how many parameters we have\n",
    "print (sum([p.numel() for p in model.parameters()])/1.0e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
