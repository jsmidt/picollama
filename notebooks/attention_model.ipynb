{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a seed for reproduceability\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model: MLPs, Weight Tieing, and batch size\n",
    "\n",
    "In this notebook, we explore what happens if we turn the final lm_head output layer into a couple of MLP layers, keeping the parameter count around 50M. We also add some complexity to the training loop such as batch size and warmup steps.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Tokenizer\n",
    "\n",
    "# 512 is a good minimal context length in principle large enough to understand paragraphs\n",
    "context_length = 512\n",
    "\n",
    "# The original Llama 2 tokenizer is available from Huggingface, but we will use the more\n",
    "# up-to-date Cosmo2 toenizer provided by Huggingface for the Llama-like SmolLM models\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/cosmo2-tokenizer\")\n",
    "tokenizer.model_max_length = context_length\n",
    "# There are times we need to pad.  We will make the padding token the same one that \n",
    "# signals end of sentance.\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033d18990e2a42d8a0c974420fbd82a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebf3216d6cc41d1a8ca183e8436cc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set dataset\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Load dataset from hugging face in streaming mode.\n",
    "dataset = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True)\n",
    "\n",
    "# Create a tokenize fundtion.\n",
    "def tokenize(item):\n",
    "    x = tokenizer(\n",
    "        item['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset.column_names\n",
    ")\n",
    "train_dl = DataLoader(dataset=tokenized_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building linear model\n",
    "\n",
    "Now let's make a linear model using a couple of back to back MLPs with Relu activation functions to explore if this is superior to just the embedding layer with linear output, assuming parameters are fixed to roughly 50M parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, n_embed: int):\n",
    "        super().__init__()\n",
    "\n",
    "        assert n_embed % n_heads == 0, \"Number of heads muct divide embedding dimension\"\n",
    "        self.n_heads = n_heads\n",
    "        self.h_dim = n_embed // n_heads\n",
    "\n",
    "        self.q = nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.k = nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.v = nn.Linear(n_embed, n_embed, bias=False)\n",
    "\n",
    "        self.proj_out = nn.Linear(n_embed, n_embed, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # (B, T, C) -> (B, T, n_heads, h_dim) -> (B, n_heads, T, h_dim)\n",
    "        q = self.q(x).view(B, T, self.n_heads, self.h_dim).transpose(1, 2)\n",
    "        k = self.k(x).view(B, T, self.n_heads, self.h_dim).transpose(1, 2)\n",
    "        v = self.v(x).view(B, T, self.n_heads, self.h_dim).transpose(1, 2)\n",
    "\n",
    "        x = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        # (B, n_heads, T, h_dim) -> (B, T, n_heads, h_dim) -> (B, T, C)\n",
    "        x = x.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj_out(x)\n",
    "\n",
    "\n",
    "# We will inherit from Pytorch's Module class\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size: int, context_length: int, n_embed: int, n_heads: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Map tokens to embedding space with dimension n_embed\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=n_embed)\n",
    "        self.pos_embedding = nn.Embedding(context_length, n_embed)\n",
    "\n",
    "        self.attn1 = MultiHeadedSelfAttention(n_heads, n_embed)\n",
    "\n",
    "        self.fft1 = nn.Sequential(nn.Linear(n_embed, 4*n_embed, bias=False),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(4*n_embed, n_embed, bias=False))\n",
    "        \n",
    "        self.attn2 = MultiHeadedSelfAttention(n_heads, n_embed)\n",
    "\n",
    "        self.fft2 = nn.Sequential(nn.Linear(n_embed, 4*n_embed, bias=False),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(4*n_embed, n_embed, bias=False))\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.ln3 = nn.LayerNorm(n_embed)\n",
    "        self.ln4 = nn.LayerNorm(n_embed)\n",
    "        \n",
    "\n",
    "        # Now map back to make prediction for next tokens, called logits.  The layer that\n",
    "        # does this is traditionally a linear layer called the lannguage model head.\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "        self.embedding.weight = self.lm_head.weight\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # Pytorch Modules have a forward method that is what is executed when the model is called\n",
    "    # The input is the (batch, tokens) tensor and a (batch, logits) tensor is returned\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        _, T = x.shape\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=x.device)\n",
    "        tok_emb = self.embedding(x)\n",
    "        pos_emb = self.pos_embedding(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = x + self.attn1(self.ln1(x))\n",
    "        x = x + self.fft1(self.ln2(x))\n",
    "        x = x + self.attn2(self.ln3(x))\n",
    "        x = x + self.fft2(self.ln4(x))\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sequential.__init__() got an unexpected keyword argument 'bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's create the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAttentionModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Let's see how many parameters we have\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m([p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1.0e6\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mAttentionModel.__init__\u001b[0;34m(self, vocab_size, context_length, n_embed, n_heads)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(context_length, n_embed)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1 \u001b[38;5;241m=\u001b[39m MultiHeadedSelfAttention(n_heads, n_embed)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfft1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mn_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mn_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2 \u001b[38;5;241m=\u001b[39m MultiHeadedSelfAttention(n_heads, n_embed)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfft2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(nn\u001b[38;5;241m.\u001b[39mLinear(n_embed, \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mn_embed, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     50\u001b[0m                          nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m     51\u001b[0m                          nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mn_embed, n_embed, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Sequential.__init__() got an unexpected keyword argument 'bias'"
     ]
    }
   ],
   "source": [
    "# Let's create the model\n",
    "model = AttentionModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    context_length=context_length,\n",
    "    n_embed=768,\n",
    "    n_heads=12,\n",
    ")\n",
    "\n",
    "# Let's see how many parameters we have\n",
    "print(sum([p.numel() for p in model.parameters()]) / 1.0e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "\n",
    "Let's add the batch size to be 250k tokens.  Like Smallest Llama models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 52.32M  params\n",
      "The vocab size is 49152, meaning the initial loss should be ~10.803\n",
      "Step:     1,  Tokens:  0.262M, Loss:  10.978, Loss_ave:  10.978, Perplexity:  58570.7\n",
      "Step:   101,  Tokens: 26.477M, Loss:   7.549, Loss_ave:   8.740, Perplexity:   1899.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 34\u001b[0m step_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accum_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"mps\"\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "accum_iter = 512 // batch_size\n",
    "w_steps = 2000\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=w_steps)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters()]) / 1.0e6\n",
    "print(f\"This model has {num_params:.2f}M  params\")\n",
    "print(\n",
    "    f\"The vocab size is {tokenizer.vocab_size}, meaning the initial loss should be ~{math.log(tokenizer.vocab_size):.3f}\"\n",
    ")\n",
    "model.train()\n",
    "step_losses = []\n",
    "losses = []\n",
    "for step, batch in enumerate(train_dl):\n",
    "\n",
    "    # Get data\n",
    "    x = batch[\"input_ids\"].to(device)\n",
    "    att_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Compute prediction error\n",
    "    logits = model(x)\n",
    "\n",
    "    # Shift to compare loss correctly\n",
    "    shifted_logits = logits[:, :-1].contiguous()\n",
    "    labels = x[:, 1:].contiguous()\n",
    "    loss = loss_fn(shifted_logits.view(-1, shifted_logits.size(-1)), labels.view(-1))\n",
    "    loss = loss / accum_iter\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    step_losses.append(loss.item())\n",
    "\n",
    "    if (step + 1) % accum_iter == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(sum(step_losses[-accum_iter:]))\n",
    "\n",
    "        acc_step = (step + 1) // accum_iter\n",
    "        scheduler.step()\n",
    "        if (acc_step - 1) % 100 == 0:\n",
    "            #  f\"Step: {acc_step:>5d},  Tokens:{batch_size*context_length*accum_iter*acc_step/1.0e6:>7.3f}M,  LR: {scheduler.get_last_lr()[-1]:.2e}, Loss: {losses[-1]:>7.3f}, Perplexity:  {math.exp(losses[-1]):>7.1f}\"\n",
    "\n",
    "            print(\n",
    "                f\"Step: {acc_step:>5d},  Tokens:{batch_size*context_length*accum_iter*acc_step/1.0e6:>7.3f}M, Loss: {losses[-1]:>7.3f}, Loss_ave: {sum(losses[-100:])/len(losses[-100:]):>7.3f}, Perplexity:  {math.exp(losses[-1]):>7.1f}\"\n",
    "            )\n",
    "            # Step:   401,  Tokens: 26.280M, Loss:   6.782, Perplexity:    882.0\n",
    "        \n",
    "    if (step + 1) % 50000 == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15a790350>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiUAAAGeCAYAAABLiHHAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAATNFJREFUeJzt3Xdc1PXjB/DXDTj2ISBLQXDiRNx7JK4wNVuammU7+5ZlZv5KK620vg2rb+nXb8OybFhqW3OPxIEb90DFATiAY8i89+8P4MMdd8c8uA98Xs/Hg8fj7rPu/TbjXr6nSgghQERERORgakcXgIiIiAhgKCEiIiKZYCghIiIiWWAoISIiIllgKCEiIiJZYCghIiIiWWAoISIiIllgKCEiIiJZYCghIiIiWdA6ugBlGY1GXLlyBZ6enlCpVI4uDhEREVWCEAIZGRkIDg6GWl3NNg9RRVu3bhWjRo0SQUFBAoBYvXq12fmff/5ZDB06VPj4+AgA4sCBA1V6fmJiogDAH/7whz/84Q9/6uFPYmJiVaOFpMotJVlZWYiMjMTUqVMxbtw4q+f79euHe++9F48++mhVHw9PT08AQGJiIry8vKp8PxEREdU9g8GAkJAQ6Xu8OqocSkaOHImRI0faPD958mQAwPnz56tVoJIuGy8vL4YSIiKieqYmQy8cPqYkNzcXubm50nuDweDA0hAREZGjOHz2zYIFC6DX66WfkJAQRxeJiIiIHMDhoWT27NlIT0+XfhITEx1dJCIiInIAh3ff6HQ66HQ6RxeDiIiIHMzhLSVEREREQDVaSjIzM3HmzBnpfUJCAg4ePAgfHx+Ehobi5s2buHjxIq5cuQIAOHnyJAAgMDAQgYGBdio2ERERNTRVbimJi4tDVFQUoqKiAADPP/88oqKiMHfuXADAr7/+iqioKMTExAAAxo8fj6ioKCxZssSOxSYiIqKGRiWEEI4uhCmDwQC9Xo/09HSuU0JERFRP2OP7m2NKiIiISBYYSoiIiEgWGEqIiIhIFhhKiIiISBYcvnhaXckrMOLttSeQX2jEyzFtodNqHF0kIiIiMqGolpLPdyTg69gLyC0wOrooREREVIZiQomTpnQr5XyGEiIiItlRTChRqVRSMMkvlNXSLERERAQFhRIAcNIUVTe/kC0lREREcqPIUJLHUEJERCQ7igwlbCkhIiKSH4WFkuIxJQUcU0JERCQ3Cgsl7L4hIiKSK4WFkpLZNwwlREREcqOwUMIxJURERHKlqFDirGUoISIikitFhRJpTAkHuhIREcmOwkIJx5QQERHJlcJCCbtviIiI5EpRocS5OJQUcO8bIiIi2VFUKOE6JURERPKlrFDC2TdERESypaxQwoGuREREsqWoUOIsDXTlmBIiIiK5UVQoKV2nhC0lREREcqPIUMLuGyIiIvlRVijRckwJERGRXCkqlHBMCRERkXwpKpRwnRIiIiL5UmQoyedAVyIiItlRWCjhmBIiIiK5UlQocdZyTAkREZFcKSqUcEwJERGRfCkylLD7hoiISH4UFko4poSIiEiuFBVKpHVKCjimhIiISG4UFUo4poSIiEi+lBVKtBxTQkREJFfKCiUcU0JERCRbCgslXKeEiIhIrhQZSvK4zDwREZHsKCyUsPuGiIhIrhQVSpy5eBoREZFsKSqUaItDSQHHlBAREcmOskKJuqj7psDIUEJERCQ3ygolmpJQwu4bIiIiuVFWKFGXTgkWgq0lREREcqKwUKKSXrMHh4iISF6UFUo0paGEM3CIiIjkRVmhRF1a3UI2lRAREcmKskKJSUsJpwUTERHJi7JCicmYEs7AISIikhdFhRKVSgUN1yohIiKSJUWFEqC0tYQDXYmIiOSlyqFk27ZtuOOOOxAcHAyVSoU1a9aYnRdCYO7cuQgKCoKrqyuio6Nx+vRpe5W3xkpCCQe6EhERyUuVQ0lWVhYiIyPxySefWD3/zjvv4KOPPsKSJUuwe/duuLu7Y/jw4cjJyalxYe1BqyldQI2IiIjkQ1vVG0aOHImRI0daPSeEwKJFi/DKK69gzJgxAICvv/4aAQEBWLNmDcaPH1+z0tqBk4YtJURERHJk1zElCQkJSEpKQnR0tHRMr9ejZ8+eiI2NtXpPbm4uDAaD2U9t0nBMCRERkSzZNZQkJSUBAAICAsyOBwQESOfKWrBgAfR6vfQTEhJizyJZKFlAjbNviIiI5MXhs29mz56N9PR06ScxMbFWP08rdd+wpYSIiEhO7BpKAgMDAQDJyclmx5OTk6VzZel0Onh5eZn91KbSKcFsKSEiIpITu4aS8PBwBAYGYuPGjdIxg8GA3bt3o3fv3vb8qGpzKp59w4GuRERE8lLl2TeZmZk4c+aM9D4hIQEHDx6Ej48PQkNDMX36dLzxxhto1aoVwsPDMWfOHAQHB2Ps2LH2LHe1caArERGRPFU5lMTFxWHw4MHS++effx4AMGXKFCxbtgwvvvgisrKy8NhjjyEtLQ39+vXD2rVr4eLiYr9S10DJOiXckI+IiEheqhxKBg0aBCFsf6GrVCrMmzcP8+bNq1HBaouWe98QERHJksNn39S10lDC7hsiIiI5UVwo4UBXIiIieVJcKNFwSjAREZEsKS6UlOx9U8DZN0RERLKiuFCi4UBXIiIiWVJcKCmdEsyWEiIiIjlRXihhSwkREZEsKTCUcJdgIiIiOVJcKHGSdglmKCEiIpITxYUS7n1DREQkT4oLJU7c+4aIiEiWFBdKOCWYiIhInhQXSrRcPI2IiEiWFBdKnDj7hoiISJYUF0o03CWYiIhIlhQXSkr3vmFLCRERkZwoLpRo2H1DREQkS4oLJdwlmIiISJ4UF0q49w0REZE8KS6UaLh4GhERkSwpLpQ4cfYNERGRLCkulHBFVyIiInlSXCjh3jdERETypLhQIi0zz+4bIiIiWVFeKFFz8TQiIiI5UmAoKapyPseUEBERyYriQommuPumkN03REREsqK4UCLtEszuGyIiIllRXCgpHejKUEJERCQnygslau59Q0REJEfKCyXF65Tks/uGiIhIVpQXStQlA10ZSoiIiOREeaGEi6cRERHJkvJCCfe+ISIikiUFhhJOCSYiIpIj5YUSdt8QERHJkvJCCVtKiIiIZEl5ocRk8TQhGEyIiIjkQnmhpHigK8BpwURERHKivFCiKa0yZ+AQERHJh/JCiUlLCUMJERGRfCg7lHD/GyIiItlQXCjRqFVQFeeSPIYSIiIi2VBcKFGpVNBpi6qdV8BQQkREJBeKCyUA4Fw82DWXoYSIiEg2FBlKdE4aAEBuPkMJERGRXCgzlJR033BMCRERkWwoMpQ4F4eS3PxCB5eEiIiISigylOi0xd03HFNCREQkGwoNJZx9Q0REJDeKDCVS9w1DCRERkWwoMpTopFDCMSVERERyodBQUjSmhN03RERE8qHQUMLuGyIiIrmplVCSkZGB6dOno1mzZnB1dUWfPn2wd+/e2vioamH3DRERkfzUSih55JFHsH79eixfvhxHjhzBsGHDEB0djcuXL9fGx1WZzomzb4iIiOTG7qHk1q1b+Pnnn/HOO+9gwIABaNmyJV577TW0bNkSixcvtvfHVQv3viEiIpIfrb0fWFBQgMLCQri4uJgdd3V1xY4dOyyuz83NRW5urvTeYDDYu0gWpL1vGEqIiIhkw+4tJZ6enujduzfmz5+PK1euoLCwEN988w1iY2Nx9epVi+sXLFgAvV4v/YSEhNi7SBa4eBoREZH81MqYkuXLl0MIgSZNmkCn0+Gjjz7ChAkToFZbftzs2bORnp4u/SQmJtZGkcyUdt9woCsREZFc2L37BgBatGiBrVu3IisrCwaDAUFBQbjvvvvQvHlzi2t1Oh10Ol1tFMOmkoGuuflsKSEiIpKLWl2nxN3dHUFBQUhNTcW6deswZsyY2vy4SpM25CtkKCEiIpKLWmkpWbduHYQQaNOmDc6cOYOZM2ciIiICDz30UG18XJVJe9+wpYSIiEg2aqWlJD09HdOmTUNERAQeeOAB9OvXD+vWrYOTk1NtfFyVcfE0IiIi+amVlpJ7770X9957b2082i649w0REZH8KHLvG2fufUNERCQ7igwl3JCPiIhIfhQdSvI4poSIiEg2FBlK2H1DREQkP4oMJdI6JQwlREREsqHMUOLEvW+IiIjkRpGhhHvfEBERyY8iQ4m0902BEUIIB5eGiIiIAKWGkuIxJUIA+YUMJURERHKg0FBSWu08bspHREQkC4oMJSVjSgAgN5/jSoiIiORAkaFErVaZDHZlSwkREZEcKDKUAKULqHFaMBERkTwoNpRw/xsiIiJ5YSjhWiVERESyoNhQwu4bIiIieVFsKOH+N0RERPKi3FDixO4bIiIiOVFsKCmZEszuGyIiInlQbCgx3f+GiIiIHE+5oaRkTEk+QwkREZEcKDaUSCu6cu8bIiIiWVBsKJG6b7j3DRERkSwoN5RwRVciIiJZUWwo4eJpRERE8qLYUMLF04iIiORFwaGEi6cRERHJiWJDCbtviIiI5EWxoYTdN0RERPKi4FDC2TdERERyothQUtp9wzElREREcqDYUMKWEiIiInlRbihx4t43REREcqLYUFKy900e974hIiKSBcWGkpK9b3K49w0REZEsKDaUuBV339zKYyghIiKSA8WGEg8XLQAgM7fAwSUhIiIiQMGhxFPnBIChhIiISC4UG0rcdUXdN9l5hSg0CgeXhoiIiBQbSkq6bwAgK4+tJURERI6m2FCi02qgVasAANm5HOxKRETkaIoNJQCg1RSFknyuVUJERORwig4lTuqi6hdwTAkREZHDKTqUlLSUFLClhIiIyOEUHkqKqp9fyJYSIiIiR1N0KHEqHuhaYGRLCRERkaMpOpSwpYSIiEg+FB5KOKaEiIhILhQdSjj7hoiISD4UHUq4TgkREZF8KDyUFLeUcEwJERGRwyk6lJTMvsljSwkREZHDKTqUxF1IBQC8sibewSUhIiIiRYeSEjez8hxdBCIiIsVjKCEiIiJZsHsoKSwsxJw5cxAeHg5XV1e0aNEC8+fPhxDyG0zaorE7AMBZw2xGRETkaFp7P/Dtt9/G4sWL8dVXX6F9+/aIi4vDQw89BL1ej2eeecbeH1cjzw9tg2kr9qNzqLeji0JERKR4dg8lO3fuxJgxYxATEwMACAsLw3fffYc9e/ZYvT43Nxe5ubnSe4PBYO8i2eTEdUqIiIhkw+79Fn369MHGjRtx6tQpAMChQ4ewY8cOjBw50ur1CxYsgF6vl35CQkLsXSSbnLhOCRERkWzYvaXkpZdegsFgQEREBDQaDQoLC/Hmm29i4sSJVq+fPXs2nn/+eem9wWCos2BSsqJrwvWsOvk8IiIiss3uoeTHH3/Et99+ixUrVqB9+/Y4ePAgpk+fjuDgYEyZMsXiep1OB51OZ+9iVIq2eO+bzNwCHLtiQLtgL4eUg4iIiGohlMycORMvvfQSxo8fDwDo2LEjLly4gAULFlgNJY7krFVJr9cdTWIoISIiciC7jynJzs6GWm3+WI1GA6NRfoNJtSbldNZyWjAREZEj2b2l5I477sCbb76J0NBQtG/fHgcOHMD777+PqVOn2vujaqxkTAnAtUqIiIgcze6h5OOPP8acOXPw1FNPISUlBcHBwXj88ccxd+5ce39UjWnUpaHENKAQERFR3bN7KPH09MSiRYuwaNEiez/a7kynAjuxpYSIiMihFP1NbLryvVrFlhIiIiJHUnQoaW8y26ZAhgNxiYiIlETRoUStVmFM52AAQF4BQwkREZEjKTqUAKVjSfK51DwREZFDKT6UlKxPwk35iIiIHIuhRMNQQkREJAeKDyVOxeuTcEwJERGRYyk+lHi5OAEAtp++7uCSEBERKZviQ4mvR9EOxceuGnA1/ZaDS0NERKRcig8lTibLy++/kOa4ghARESmc4kPJyI5B0mu2lBARETmO4kOJh06LCT1CAQBZuYUOLg0REZFyKT6UAICHTgMAyMorcHBJiIiIlIuhBICHrmgGTkYOQwkREZGjMJQAcC9pKcllKCEiInIUhhIA3m7OAIDdCTccXBIiIiLlYigBMKCVHwAg2ZCLz7afc3BpiIiIlImhBIC/l4v0+o0/jjuwJERERMrFUEJERESywFBSbET7QABF65Zk5hag0CgcXCIiIiJlYSgpNjaqCQAgM7cAHV5dh8e+jnNwiYiIiJSFoaSYp4vW7P3GEykOKgkREZEyMZQU89BpLY5l5xVg59nryMkvxL4LqRixaBv+OnLVAaUjIiJq+Cy/iRXKw8Xyj6Ld3HUWx578dj/OL4ypiyIREREpCltKinlaaSmx5XRyBnacvl6LpSEiIlIehpJi1lpKbBn6wTZM+nw3dp61HUxu5XHHYSIioqpgKCnm6qSp8j33/283hCiaOhx3/ibWHLgMAJizJh5t567F0Svpdi0jERFRQ8ZQUkylUkmvVz7Ru9L3bTxeNEvn7iWxmP7DQdz/v11YvusCAODjjWfsW0giIqIGjANdTWx4fgCuZ+ahe5hPpe955Os4RDbVS+93ni3d1E+jUVm7RXIl7RbO38hCz3BfaNRF1xYUGqHVMCsSEZHy8NvPREt/T/Rq7gsA2PLCoErfd+iS9W4ardo8lPyw9yJ6vbURx68acCk1G30WbsL9/9uNxVuKWlQupWYjav56vPjTIdz+4XYs2Xq2ehUhIiKqhxhKbAjzc5dej+oUhGeGtKryM5LSc8zez/r5CJIMOZi96ojU7QMAX/xzHgCwaMNpZOQU4Me4Szh21YCFf52o8DOMZZbDX3c0CXsSbuJWXiESrmdVucxERESOwu6bShjQqjHu7R6CDsFeeGz5vkrftzvhptXjBxPT0KeFr/TeWDxYNjuvoNznnUrOgJNGjXA/d/x9NAkHEtPw68Er6NhEjyWTu+LijWw8Xly+5n7uOHc9C60DPDBvTAepBYiIiEiuGEoqQe/mBAAY1j4Q9/cMRYohBxq1CuuOJlf7mV8Wt44AkDb/y8q1nEY895d49G3ph74t/TDsg20AgLNv3W4Wji6n3UJadh6Wbi/t7jlX3EpyKjkT45fu4oJvREQkewwl5Vg4riMOXUrD0LYB0rG37uwIAEg25FQqlHyy+Qz6t/LDmZRMs+Om400ycgpwMDENWbmWLSVfx17A17EXsHHGQOlYanaexXXjPt0pBZHqOHstE1OX7cUTA1tgQo/Qaj+HiIioulSiZKENmTAYDNDr9UhPT4eXl5eji1Ou/1t9BCt2X6yTz7otwh+bijcJXP1UH9z56c4q3W+rpWRtfBIMOfn488hVbDl5DQAQ//pwq3sBERER2WKP728OdK0B07k1SyZ1rdXP2mSya/HRK4Yq37/zTOnqsyU5dNe5G3jim3148afDuHgjWzr/77UVD7AlIiKyN4aSGjCd+DKoTeM6+9xX1sRX+Z77P9uNxJvZ+M+m0+gyfz3+OXMd45fuks6bdv3UpBuIiIiouhhKasDLZL8cFycNdswa7MDSVOzMtUy8+/cppGbnY+Jnu21e5+3mXIelIiIiKsJQUgNRod5m75s2cnNMQSqpsuNEnLmiLBEROQBHM9bA8PaBmDuqHTo00Vd8sQyUXWjNlvxCI7afvoaQRm5mi8gRERHVJoaSGlCpVJjaL9zRxai0TSdTKr4IwMbjyfj10BUAwMgOgXh8YAt0DvGuxZIRERFxSrDdnb+ehUHvbnF0Mezutgh/PNgnDH1b+kmbBxIREZXglGAZCvNzR/PG9uvy6NqsEQa2tpzZ07+Vn90+ozI2nUjBA1/swSebz9Tp5xIRkXIwlNSC/0zoYvb+zqgmlbrvhWGtLY492CcMQ9r6Wxwf07n0md7Fy+BXx9S+4Xh9dPtKX//++lPV/iwiIqLyMJTUgnbB5s1Wb9/VyeKaLmVm7gDAI/2bS687h3jj93/1w6hOQQj0crG49q4uTfD2XR0xoUcIfpnW1+zcjKGW4cYWFyc1hrUPqPhCIiKiWsZQUgectZZ/zNZWgHUymYr73NDW6NBED5VKhei2AZg5vA0+uC8SAV46jOvSBCqVCvd1D8WCcZ3QzNe8u6hPy8rvCKxWqTgFmIiIZIGzb2rJh+M7Y/aqI/h0YheLcxN6hMK/TOuH3tUJGrUKk3qFIjOnAP1alo4ZUatVmDa4JQBgTGQTqK0MNG3i7YrLabcAAF4ule/OERDQOWkqfT1QtDx9r+a+SMvOg6eLEwe+EhGRXTCU1JIxnZtgVKdg6Qt76eSu2HHmOp4Z0gq+7uYrpr4S0xbji3fmfWNsx3Kfay2QAEBMpyAs3XYOANAqwBOPD2yO1Kw83NWlKbaeuoZPt5y1el9WbmGVW0re+/sk/n13JAa9uwW9mvvg+8d6V+l+IiIiaxhKapFpC8Kw9oEY1j7Q6nUDWjeu8a68ZcedzB7ZVnrt7eZsM5Rcy8iFk6a0nF2bNcK+C6nlftbe86mYveoIAGDXuZsW5wuNArfyC7nTMBERVQkHEzjQ+ucG4JuHe6J1gGeNnzWxVygm9QrF51O6WZwzDR1jOwebnbu7a1OoVKXnP7nfsrvJmthzN2yem7psL/os2Ch1JxEREVUG/ynrQK0CPNHKDoEEAHRajc2uH9MBtJ2aemPNwaLVWv9+boAUiNY/NwCGnHwE6i1n+lRECAGVSoVLqdn488hVbD11DQDQd+EmbHlhEJeqJyKiSmFLiQKYhpIQn9JNA01baFoFeKJrMx8AgKdL1bLqfzYVLah256c78dafJ8zOjf7PjiqXl4iIlImhRGF6hPvg/26PwGcPWHbzlFj+cE+0CfDE11N7oJW/R4XPfG/9Kaw/loxrGbkW5ww5BTUqLxERKQe7bxTAw6TlQ6dV47EBLcq9vnOIN9Y9NwAA8M0jPTHt2/2Iq2Dw66Nfx9W8oEREpGh2bykJCwuDSqWy+Jk2bZq9P4oqyUOnxXeP9sKPj/eGSxXXJAnwcsGDfcNq9PmLNpzCzJWHkGzIqdFziIioYbN7S8nevXtRWFgovY+Pj8fQoUNxzz332PujqAp6t6j8Kq9l+brrpNdD2wVg/bHkKt2/aMNpAMDKfZcQ0ykI790TWeVwREREDZ/dQ0njxuY72i5cuBAtWrTAwIEDrV6fm5uL3NzSsQgGg8HeRaIa6tXcB9MGt0Arf0+MjgzG/D+O4ct/zlfrWX8cvoq+Lfxwf89Q+xaSiIjqvVod6JqXl4dvvvkGU6dONVsLw9SCBQug1+uln5CQkNosElWDSqXCzOERGBtVtMR92RVpqyonv7Dii4iISHFqNZSsWbMGaWlpePDBB21eM3v2bKSnp0s/iYmJtVkksoOadr146LQoNApsPpGC/1t9BJfTbsFoFHYqHRER1VcqIUStfRsMHz4czs7O+O233yp9j8FggF6vR3p6Ory8vGqraFQD3+6+gJdXx1f7fl93ZxQKgbTsfOnYyA6BWGxl52RTGTn5+GD9adwRGYSo0EbV/nwiIrI/e3x/11pLyYULF7BhwwY88sgjtfUR5CAtGle8dkl5bmTlmQUSAPgrPqnC+977+xS++CcBd366s0afT0RE8lRroeTLL7+Ev78/YmJiausjyEF6hvtgat/wOv3MPw5fxbKd5+v0M4mIqG7VSigxGo348ssvMWXKFGi1XJ+toVGpVJh7RztEt/W3+7P3XUjFD3svWhyftmK/3T+LiIjkpVYSw4YNG3Dx4kVMnTq1Nh5PMvHa6Pa4mZWH/RfT7PK8p1fsx++HrwIAQn3cpbVVEq5n2bxneex5fLPrIr6a2qNamwkSEZF81EpLybBhwyCEQOvWrWvj8SQTTRu5YdVTfbHsoe4AAHfn0lk588d2kF6P6hRUqeeVBBIA2HHmGm5kFq1fM/jdLTbvmfPLUZxMzsAH609VpehERCRD3JCPamxQG3/Ezr7NbPaMi7b0r5be1anKz/xk81l0fWODFEwqUsApxURE9R5DCdlFkN4V7rrS3kBXk1aT6oSSEl3f2GD1+P3/24UNJsvd65xK/ypn5xXgieX78ONernlDRFSfMJSQ3biaLKpm+romocSWnWdv4BGTnYlX7L6IdUeTUGgU+HjTGaw9moQXfz5s988lIqLaw1BCduNs0mXjUsuhxJrHl+/Dj3GJOHwpzeLcmZRMbD6ZgiOX0uukLEREVHWcr0t246Qp3d/ItCvH06VuQgkAbDqRgn/O3DA7tvPsddz/v93S+1NvjDQLUNZk5hbgUmo2IgK5qjARUV1hSwnZjWn4aNHYXXotYH0Q6su3t7V7GdabjDMBgMmf78YXO86bHYs9Zx5arLnj4x0YsWg7dlXiWiIisg+GErIbH3dnzB/bAQvGdYSnixP+dVtL9Aj3wYDWja1e/+iA5rVepu2nr1t050z5Yk+59wghpLVRfjt0pbaKRkREZTCUkF1N7tUME3qEAgBmDGuDHx/vDQ9ny17CV2KKWkkGtbEeWOwpNTuvStevjLskvf52t+XqspVxIzMX9/03Fj/vu1TxxUREBIChhOqAWq0yez+ldzM83K9o75wlFewMbM2YzsEI8NJV+vr8Qsvuozd+P4awl/5A2Et/4Oy1TLNzX8Wer3KZynp//SnsTriJGSsP1fhZl1KzseP09Ro/h4hI7hhKqE6NjgzG62M6QKUqCiqms3Qq68PxUQj2dq1ROT7bkSC9HvLeVum1IScfR68YKvUMo1HgP5tOWx13knardBfkdUeT8PSK/Yi/XL2ZP/3e3oxJn+/m+BYiavAYSqhOZecVVvrakuXrrTHaeQXXx76OQ/qtfCz757zV85m5BVi+6wJSDDnYeuoabntvC15YeQjv/n0K45fuQnZeAf44fBWzfjqMnPxCmLYNPb58H34/fBUL/zpRozLuPnezRvcTEckdpwRTncrOK7A4Fu7njoTrWfjfA90we9URXC9eWn5QG3/4ujvjRlYeFt3XGdN/OIhW/h4ArC8rr1GrUFjNsPL3sWT8/frfVs89sXwf1h5NAgDMWRMvHT93rXSjwMHvbkGyoajcob5uUkuQqR1natYFUyhK6yaEsPoZRET1GUMJ1Ym7ujTFz/svYdrglhbnfn26Ly7cyEb7YC/kjm6H2auO4JP7uwAANr0wCFfTbyEi0At9WvrC29UZAJCTX9ricn5hDDJy8uGh0+JS6i30f2ezXcteEkjKUxJIAODf607a9fNLlLQObTt1DdNW7MfCcZ0QU8nNDomI6gOGEqoT/767E14aGYHGnpYDVD1dnNChiR4AMKpTMGI6BkmtAHpXJ2lFWH9PF+me3AKjxTMAIMTHrVbKLwfG4paSh5btRaFRYNqK/YjpFOPgUhER2Q/HlFCdUKtVVgOJNZXplsjJN9o817u5b6XLVddM10zJzC3AvN+O4WBiGoQQWLzlLLacTJHOFxqF2dgZaz1TZ1IyLQ8SEdVTDCVUL+Xm2x4w+80jPdG0Ufmzc6ozFdkezl7LxOc7EvDTvkvo8Oo6fPFPAsZ+8g+2nLyGt9eewINf7gUA3MorxIB3NuPBZXule0taSpw1pf/bRr+/FTtrOFaFiEguGEqoXrqra1MAQK/mPhbnNGoVvN1s77fz6cQuGNEhsMLP+HhCVPULaMNzPxzC/N+P4YUy65eYro2y/lgyxn7yDy6n3cK2U9ek4yWDeMvu27PqwGW7l5OIyBE4poTqpZdGRqBnuA/6tvKzer5dkBfiL5euNxIZ4o1DiWkAgNs7Vm5waCM35xqXs7K2nCwNH49+HWf1mpKWEieNeSg5lJiGw5fS0Kmpd62Vj4ioLrClhOolFycNRnYMgpeNHYhfvr2d2Xudpvy/6nd1aSq9dtaq0TrAAz3CLVthHOnLf85j55nr0JVpKTmdkonR//lHmpEkhH3XcCEiqisMJdQg6ct03zhpbQ+e9ffUYUKPEOn9fyd1xbrpAyy6SSoyshJdQjV1/2e7cTntltVzN7LycCXtFrq/uQFvr63ZQm1ERI7AUEINlumWO60DPC3Or3qqDx7uF47NLwwy6xLRqFU2ZwCNaB+I/9wfhdVP9bE4t3hSVzQyCUMfTYjCuC5NalCDqum7cBP6LNyE65l5WLzlbJ19LhGRvXBMCTVY3z3aC7NXH8G80R0QGaJHboERd3QKls53CW2ELqGNAABaTWkI0aotA0mv5j54tH9zDG7jL20wuGPWYPR723yhtmUP9cCYT/4BAHRqosfoyGB46rT4KvaCxTNdnTS4Vc4sInsRQiAztwCD392KnPxCxL0SDRcnDRZvOYttp66hR7gP7useYrafUGZuAdycNBabKVbX0Svp2Hg8BY8NaF6t/Y6ISBkYSqjB6tncF5tmDJLev3VnR5vXOpdpKSnL39MFQ9oGmB1r2sgN7YO9zDbwM21xKQk6r4/pAI1ajS/+STC7v32wF+IupFauMtV0MysPoz7aDmetWlq+f9X+y5jQI0Tq4ok9dwOfbT+Ho/NGACjalbjf25txW4Q/vnjQ9v5DVRHz0Q4ARYN1p0e3tssziajhYSghAqC1EiZM2WowKNvL42wydsU0oAhYDj69t3tIrYaSXm9tRJIhx+L4W38ex8p9iWbHsvIKUWgU0KhVWBl3CQCw6USKxb0AsGr/JXyz6wKWTOoKfy8Xq9fYUjIDiojIGo4pIQLgZBJE1FbGk1g7BgBNvM0XadOqTcKNSZIxnRBzfN4I/Pxkb9zTtWmFi7zVhLVAAhR1zRy4mGZxvMv89Yi/nI4PN542O77/YioOX0pDiiEHKRk5eP7HQ9h/Ma1aux4XcmIQEZWDLSVEKNPtoraS1W20lLw+ugPyCwUe6N2s6DKT67Q2piG7OmvQtVnRdONfn+6HPgs3lrtsfl1Jv5WPUR/vMDt2MikD4z7dafP6qqrqdOW8AiOOXE5HZFO9zT9PImo4+H85EcxDiWmwCPEpaskYZWM33kC9C754sDsGtfEvuhem3TemLSXWv4x93J1xYv5ILH+4h3RsSIQ/jr4+vOqVqAXDF22zea5kMbeVcYlYuu0sDl9KQ9+Fm/Dt7qJBvZtPpODLMuNojELgVl4hNp1INtvp2ZaXfj6MuxbvxPvrT1k9n5Seg00nkrk2C1EDwZYSIpiPIzH9fvvjmf5IuJaFTk31lXqOq3PpzBLTAbPdw32szsAp0b9VY+n1+B6hcNdp8UDvZvi6nHsczSiKwtbMnw6bHX95dTwm9myGh4r37Wnp71F6jxF4ec0RrNp/Gfd1C8Hbd3cCAGTnFSA7rxB+HuabNpYsob9461m8OCLCogz93t6EAqPApxO7VHqlXiEEvtp5HhFBXugl480biZSIoYQI5rNvTAelerk4ITLEu9LPaeypw9xR7eCsVUOnLQ0oMR2DgPuBjk1sh5s3xnbA0SsGDIkoanWZ0icMX8dewPD2AVh3NLkKtakbcedvIq/QerfTpdRs6fXcX45Kr2PP3ZBe/xCXiAXjOuLBZXulPX4OvzbM6iq9tsb0FBTvB7T99PVKh5Jtp6/jtd+OAQDOL4yp1D1EVDfYfUME80Gpxhr2BEztF45JvZqZHVOpVBjVKRjNfN1t3jepVzMsGNdRWhukRWMPxL8+HEsmdbUIRj3CfLBxxkCzY8183WpW8CrKyitEm1fWWj1nun7L1XTrK9ACQPP/+9Ns08Fz17KsXmc6+8lY/B8or6A0ENnILDh3LRMjFm3Dr4euSMcSrmXaLA8RORZbSohg3tVilNH4BA9d0f+iZcdM6JzUaNHYA79M64sLN7MxOrJoUbiwl/6o0ecdmzcc7eauq9EzyqrKIF7TfX3OmYSH/EKBn/ddwsA2jTHyw+3QqFRIySidXWQtk3yxIwHzfi9qEXnmuwPSnxFnABHJF0MJEWC2rHxYOa0ZjhLZ1BuHL6VL7w05BUXHQ7yr1L1UnmC9C9ycHfsrYcFfJ3DgQioycgssVtadsfIQHhvQHNcyci3uK9tSEn85XQokJfaevwkhbA86NhoF1GoVtp26hm92XcAbd3aAv2fV1mEhopphKCEqtvv/huBWXiF83J0dXRQLs0ZGoJG7Mz4qXkNkQCs/uz6/eWN3LJ7YFQAwY2hr/BmfhD4tfPH5joQK7rQv066cAiv9aIk3sy2OAcCpZPMuGWubFt6zJBYAMKB1Y4tzL6w8hO2nr+Hv6QPxwBd7ABTtRP3RhKjKF56IaoxjSoiKBXi5IMxPfq0kQFE3zvNDW2PX7CF4+66OmDa4pdXrfnu6X7We//1jvdAmsGjTwn8NaYW/nu2PsZ3rbjPByvorPsnq8T0JN2E0CqyMS8T561lm403KMg0+hpyitVZ+2ncJyYZcfLf3onQuKd364nMlXv/tKIZ9sBVZuQVVqQIRlYOhhKgeCdS74L7uoTY3tevYVI+NMwaiYxM9lkzqggCvoim2IzsEStd4u1nObjGdKWT6rF+m9bVTyWvfT/suYeZPhzHo3S1WW0qsubN488QSySar4KrVQEpGjjSwtsTmkykY+eF2fPnPeZxKzsQnm88g/nI6Fvx1HBk5VV9QjohKqYTMVh0yGAzQ6/VIT0+Hl5eXo4tDVK8ZcvJxOfUWIgI9cSn1Fpo2coVKpcL3ey7ipVVHpOsSFtxuNq7GVIdX1yHTRmuAWlU0WynUxw0XbXStyN35hTHSAOF2QV44dtVgcc3a6f0REVj0+6i8wcQP9wvHnFHtABR1NcVfTseIDoE2/2yJGhJ7fH+zpYSoAfNycULbIC+oVCqE+LhJX46m635M7tWs3C/NTTMGYtlD1ncLPj5/BD57oBt+frIPXolpW+lyfTi+c6WvrW2mq8VaCyQAMOY/RS0qadl55T7rdEomcvILsfF4Mvq/sxlPfrsfvx++anHd5pMpOGIycPnijWysjb/KlWlJ8TjQlUiBTDPI/LEdyr3W38vF5m7AOq0G0e0CAACP9G+O1Ow8fLL5bIWf36KxBzqHeOOgDHYN/qjMBoTW5BYYsTY+CU98s6/c65LTczD2k39wIilDOrb3/E3cUTwdef2xZKw+cAl/HikaG/NKTFt0C/PB2OJupCWTumJYuwCsPZqELqGNEKjn7B9SFoYSIgWytUJqeba/OBizfj6MnWeLVmW11jJibWxKid3/NwQ939qI6Lb+aB/shf890A1Ltp7FiSQD/jlzw+Z9clFRIAGAk8kZFsc0ahVe/SUee86n4niZlpg3/jhu9n7rqWu4kZWLl1fHw9fdGfvmDK1ZoYnqGXbfEClQk0auVb4nxMcN43uESu+tLevu4lT6K+XD8Z0RZPIv/QAvF5xfGIPPpnSHSqVCY08d5oxqh28f6VXlstQnTho1voq9YBFIrMnMLcDLq+MBADeyyu8qqsiNzFy8+ks8jl2p+HOJ5IKhhEiBeob7YPbICHzxYLcq3edusuFg2cXNAOC+bqFo4u2KB3o3w5jOTdAuqHYGq+986bYKrxnUxnI9EkdYuu1cpa9dd9R8ynNmboHVUJFsyMHUZXvx55Gr+Pe6E4i/nG52/mRSBrq+sQFfxV7A7R9tr17BrTDk5GPmykP458x1uz2TyBS7b4gUSKVS4fGBLap8n+kuvhoroUTv5oQdswZLA2dtbdhXE48PbI5gb1eE+7kj4br1vXLGRTWxuvia3JVdX2XumnisOnAZ88a0xwO9w7BowymkZuXhWmYuNp1IwaYTKQCATzafxak3RkKlKmqZGb5om8Wzz1/Pgr+XTlq1N/5yOr7dfQEP9Q2HVq1C88YeFveU9cH6U1i57xJW7rvEzQypVjCUEFGlma52a2vGjulx03U/qsPLRYv8QoFb+YXSsdkji8ay/Ph4b5y9lomvdp6XFlVr2sgV218sCkUv/Xy4Rp8tB6sOXAZQtNOyEMCiDUWDcp00ln/2A97ZDK1GhReGtbE4d+yKAbd/tB0RgZ5YO30AAGDUxzsAAN/tSQQAbJwxEOG+7tKGkNZcuFE/p31T/cHuGyKqtACTWThuzrYHtZaICmkEwLyFxZoH+4QBKN2AsETzxh747+Su0vtOTfXS68aeOvRq7mt2/XeP9pJCUUTxCrWVYVqXisrqKK/+elR6nW9lV8EkQw4upd7C9B8OWpz77XDRLsmms4LK+u/Ws4ic9zfCXvoDc9bEV1ielBoGTiJrGEqIqNKctWrEzr4NO2YNtrmqrKnZt0fghWGtsWZan3Kve/WOdtj7cjQ2vzDI7PitvEJ4uZauQPvDY70t7g3Slw7aDfFxk15P6tUMPcN90LxxxVsHFJp09Sx7qDu2vzgYfh7O6NfSvnsMOYpp28fMlYewYvdFi2t+jLuEjOKNHpfvumBxPr/QaNbyVbJHEJE9sfuGiKrENARUxNvNGU/f1qrC60pm45Rdpj0ztwCRTfV4alALNPN1g6uV1plno1vhWmYu7owKNjuu1ajxw+NFISbxZjYycwsw8kPrgz6DvV2l8SlaTdFCc3GvFE3HLW8F17Ie7BOGZTvPV/r6uvLpltK1Y0rGhFTFgYupePGnwzidUrrxYdlWl6T0HOQWFMLPQwd3Hb9aqHr4N4eIZMPTxQkfTYjCM98dAABk5RVApVLhxRERNu/Ruzrh4wp28zVtQQGALx7shpNJmQj3c8cXOxLw+pj2UmCxtUt0cz93pN3Kx81ypuq+Nro9RnQIxPilu8otT30ghIBKpcLhS2m489OdFV7fa8FG6XWglwu6hjXCJ/d3qc0iVtlP+y7hRmZutQZ5U91gKCEiWRkdGYzYszfw3Z6LeHZIxa0sVfHLtL64lpGL2yICcFtE0Uq0I4o3K/zv5K7IyS+Ev6f1VVTVahV+mdYX565nYUpx18XXU3tg2c7z0iwYwDIA1VeFRgGtRoUf9iaWe93KuESzgchA0fiWPw5fxX8mCIsB0XkFRhy9ko7mjT0w7dv9GNDaD48NqJuQ8MLKQwCA6HYBaFGJ2UZU9xhKiEh23hjbAQ/3C7P7F0dkiLfNc8PbB9o8BwBGo0CIjxtCfNywZFJXGIXAgNaNLbpCgq0sDd8l1Bv7L6ZVqoxeLloYcqxvgFiXZv50GLNGROBbK+NPSpy9lomZP9me5ZRfKOCsVUmtLgAwe9UR/Lz/EiKb6nHoUjp2nLmOu7uG4NClNESFeMPbzXpLFQAUFBqRlVsIvZsTTiQZ8NuhK3hiYAt4uljufF2W6W7PmTL48yXrONCViGRHo1ahpb+nrHbXNZpsljeiQ6C0oq1bmQG/KpUK218cbHZs2dQelf6cA3OHlbthoZ+H7S9te1p94DLe+vN4udcMeW9rueeTDTn4Ye9FdHztb+xJuAkA+Hl/UYg7ZLIh4QfrT+GhL/ei87z1WPZPAs6kZJptTpiWnYcJS3eh5ct/IXLe37icdgsjFm3HJ5vPWuxdlJadh6nL9uKPMhshmq6ZY7rNwvXMXLz390kk1tNdrhsahhIionI80LsZANgc19LMz7K7JsTHDU28SwcEe7k44YfHbC+nb7oQnUatQnmbBa+dPgBbysxSqi1lV4qtqie+2YdZPx9BZm4B7v1vLJLSrU8jNp3t89pvxxD9/laEz/4Tu87dwC8HL2PJ1nOIPVe6P1LfhZuk1//bnoCoeX9Ly/gv3nIWm06kYNqK/WafkWuyMF1OQWl303M/HMTHm87gvv/G1qiuZB8MJURE5Xh9dHvs+b8hVvf6AYCpfcMxLqoJPp1oPqjzsynd0CPMBz8WzwDqEe6D6dGWY2S6NWuEF4ebL3gmYDuV+Lg5I8zPHeuKF0GrTdcyc2t0/9EyS+SXjOmorPFLd+HZ7w9ix5lr5V6Xmp2PkR9ux12Ld+Ibk4CTeDMbj3y1F7vP3TBbLfeeJbGY8sUeCCGwo3jJ/Cs2AhPVLY4pISIqh0qlgr+X9cGvAODipMH793W2ON42yAs/PlG6ropKpcL06Na4nHoLGTkFWGuyz42Hi/mv4vJaSkpWXG0T6Gk2/qQ2xqJk2Pl5O6q5Z0785cptKrjvQqrZ+2e/P4D9F9Ow4XgKQnzMp7JvPXUNdy3eWe6fdYms3AJsOpGCwRH+Fgv8AcDp5AxsPXUNk3s3K3en7JKVdQO8dPjtX/0sBlV/t+cifN2dMayC8U0NGVtKiIjq0L/vicQSk1VqAeCuLk0R3dYf88a0B2AeSgaXs7HgH8/0xysxbbF/zlAcfm04nhxkexZLTMegcseqVFZUqDd+ftJyETs5Mh1cnHjzVrnnTS37JwGPfh2HvAIjTidnoP2r6/Cv7w6gw6vr0HfhJmw8nmx2/dAPtuGNP47js+0J5ZanZHPEZEMu/rPpjNm5hOtZmL3qCB5bvq/Cep1KzsBt723Br4euVHhtfVMroeTy5cuYNGkSfH194erqio4dOyIuLq42PoqIqN5zcdLgsynd8UDvMADmg2o/NlnrY1Qn8y6kEB83PNK/ubS2yqwRETaDh4DA6Mhgq+eq4qupPdAltFGNnyNHN7PyMO7Tf/Dab8ew/lgy1hy8jJdWHTG75nLaLTz8VRwe+WqvxYaQ+4tbauLO38TMlYeQWs6aNlfSSkNSXoHRbLyN6SDfVfsvYcGfx82OTf/+IM5dy5LW82lI7N59k5qair59+2Lw4MH466+/0LhxY5w+fRqNGjXMv8RERNURrHfBlfQcq1ORTZfW15oMgp0e3brC59qasZRXYLlmSHW4aDXlPqdPC1/sPHvD5nk5m7B0F04ml65U+2I50503HE/BhuMp+P1f/aRjN4pDyN1LigbN5hUa8eF46wv7bThetLZNVm4BBr27BdcySsfvZOQWYN+FVPQM98HzPxaNw+nfqjH6tSra9sBQZuVjU/mFRjz3w0F0D/PBlOI9peoTu4eSt99+GyEhIfjyyy+lY+Hh4fb+GCKieu3Xf/VD3PmbiG4bYHEuum0A7u7aFJ1DvKHTqtE6wAMZOQVo5lv9hdnyTabEAkUr1J4z+Zf+c9Gt8dn2c8jItT2OpEVjd2mH4u0vDkb/dzZbXPPNwz3R/P/+tPmM/q38sP109caW1DbTQFJZT31bOsvnYGIaNpsspLcn4Sb2X0xF7NkbeMLKKrJX0m7hZHKGWSABgBk/HsL6Y8kI8CrdHPK6yaBj0+69+b8fQ0pGLga2boy7uzbFb4eu4PfDV/H74au4LcIfyYYcdAvzqXK9HEUlRGWG+VReu3btMHz4cFy6dAlbt25FkyZN8NRTT+HRRx+1en1ubi5yc0v/sA0GA0JCQpCeng4vLy97Fo2IqF4qKDRCAHDSVNzjHnf+pvQvdR93Z2lZ/D4tfLHi0V64mZWHjJx8hPq4IXx2aXg4vzAGqVl5iJq/3upzIwI9sWZaX7ONGJ//8SBW7b9sdt35hTFYvOUs3l57wupzlj/cA1fTc8pthWgoXJzUyMkvCoPtgrxw7Kr5gF03Zw2y8wqt3Wrh3XsicXfXpgCAXm9tRJKVXZrPL4zBxxtP4731p8yO//6vflgZl4jb2gZgYGvbY5RqymAwQK/X1+j72+5jSs6dO4fFixejVatWWLduHZ588kk888wz+Oqrr6xev2DBAuj1euknJCTE3kUiIqrXtBp1pQIJAHQL88ErMW3xxYPdsM1kEbdGxSul+rg7o5mvO1QqFeaMamd2byN3Z9xhY9zJ7R2DLHaGnja4pdVr3Uw2TmzsqTM718jNGcPb1f7sEtO1XxylJJAAsAgkACodSACg0Fj0rM0nUqwGEgA4dy3TIpAAwKiPd+Cr2AuY8sUeZJXTEiYHdm8pcXZ2Rrdu3bBzZ+kGTs888wz27t2L2FjLxWnYUkJEVHvWxifh69jzeP/ezgi0sgT+8tjzaOnvid4tfAEAOfmFiJizVjr/+ZRu2HbqGl6OaQdnrWUwyskvRGp2Hp5Yvg93RAbjkf7N8dn2c3jjj6LVYM8vjMFHG0/j/eIvy/jXh8PVSYMW5XTxmOrX0q9aU4nnj+2AOWviq3yfXL11Z0d0C2uEYR9sq/GzxncPwcK7OtmhVOZk2VISFBSEdu3M03fbtm1x8aL1/RN0Oh28vLzMfoiIyD5GdAjEikd7WQ0kADC5d5gUSICimUBRod7S+yFtA/D6mA5WA0nJ9UF6V/zydD880r85gKLN/EzFFM8aah/sBQ+d1qwV47U72mH1U30QY2NxOned7XU/pke3wtm3bkcPK2MmujWz/+SKTk31dn9mZRUKged+OGiXZ32/N9FsjIqc2D2U9O3bFydPnjQ7durUKTRr1szeH0VERLXA2gJhVVFYpgG+RWMPbH5hEJY/3NPq9VGhjfDJxC7Y8/IQ3FM8bqJEeYuR3dstBBq1Cj883gtbXhiE1+4o/Qdx2yAv/PPSbRjaznIgsS1lu5rKeu+eSPQvngFT1+asibdYIbcm5LrXj91DyXPPPYddu3bhrbfewpkzZ7BixQosXboU06ZNs/dHERFRLZg/pgM6h3hjyaSuFV9sReem3hbHwv3cpfVUAGByr2Zo4u2KO7uUhhB/Txe0CfQ0u8909nEjt6Kp0i5Oavz+r34ILt5fSKVSIczP3WIcSRNvV7w5toPZsYGtG2PHLPMNE0tsnDEQLwyzPe26ma+7zWBV31xJk+ey+nafEty9e3esXr0as2fPxrx58xAeHo5FixZh4sSJ9v4oIiKqBWF+7lgzrW+17+/T0g9LJ3dFS38Pm9fMH9sB80TFa6eYnl0wriPaBekR4uNq/T4rx/y9XHDmzZGIPXcDnUO84eniZHlfMU+dFnd3DcG7f1sOFn1+aGupC+u9eyIxo4r7+MhNarbthd0cqVb2vhk1ahRGjRpVG48mIqJ6oDL7t1RmMTe1SoV374nEwcRUDG0XWO6sGluntBo1+reqeCqsSqWyOvZmUq9QPDOkdDPFu7o2RbC3K15ZcwRnr2VZXF8f5ORXfuZPXeLeN0REJF8q4O6uTfHG2I4VTvPt37IoeHi5VPzv7U8ndjHbpC/SZBDr11N7SK8/uC8Sb4ztaHF/7xa+mHtHe+n9ikd6YpGVjRkrq19LP9zfM9Ts2Dt2miGj06rha9J1Bsg3lHCXYCIiko2yrScqVH69kVBfN2x/cTAalfkCtub2jkGIDPFG34WbAADLHykdKzKgdWOcXxhT4TMGtPLDjKGt0SrAE31aFg2AnV7NGTLuOg3eurMjVuwunak6vH0gYjoFIfr9rbiaXv0xIF9N7YGXVx+RlsEHzNdQkROGEiIikq2Swa2VFeJT+aX4PZxLvwJdypnlY4tKpcK/TLp1aqJkGvWrd7TDog2n8fXUHtAX170mq4mdmD8CLk4adGvmY9bVdIstJUREROULaVTapdKnhS/+dZt9vvSt0bs5YenkrnDWqm2uw1JVyx/ugVd/PYqXRkTgctotvP7bMZvXmu4/VFAcSh7qG44H+4SZtRgVGKuXSrqEekur8P7f7W3h5+mMCzey8fvhq+y+ISIiqsjQdgGYObwNOjXVV2pwak1VZkBuVfRv1RibZgwCAOy/mGr5ee0C4OGixYN9wuDmrEX0+1sBAAWFpcGjbBeWsZpNJff3LF0fTO/mhJnDI7Bk61n8fvgqW0qIiIgqolKpbO6pU99orQzM7dncFw/3C5fel+yaPLm37QVGCworHv9huvlfE29XXE67ZXWhN5fiFiG2lBARESmIl8maKCuf6I0tJ1MwuZd5+Ph8SndcSs1G88a213R5bXR7PP9j0boo/53cFbvP3UTTRq6Y93tp19CTA1vigw2ncG+3ppg3pgMycwvg52G5Qm3JOJW07Pwa1a22MJQQERHVgjA/d8wY2hp6Nyd0D/NBdyt79Dhr1eUGEgAY16Up+rXyQ2MPHVQqFYa3D8SNzFwplCye2AVD2wWgXytfdGiih06rsdjRuYS/Z9E6LNcy5Ln3DUMJERFRLbHX7JySMFHCdM0Wfy8XaDVqdG1mGXrKKtnfJ0WmoYSLpxEREdUzapNQUomFcSX+xaGk0CiQVyC/tUrYUkJERFTPaEySSBUyCfSuTjj6+nC413An6NrClhIiIqJ6RmPWUlL5WKJSqWQbSACGEiIionqnon2A6iuGEiIionqmut03csdQQkREVM9Ud6Cr3DGUEBER1WMl03wbAvmOdiEiIiKbvnu0Fww5+QjSu1Z8cT3BUEJERFQP9W7h6+gi2B27b4iIiEgWGEqIiIhIFhhKiIiISBYYSoiIiEgWGEqIiIhIFhhKiIiISBYYSoiIiEgWGEqIiIhIFhhKiIiISBYYSoiIiEgWGEqIiIhIFhhKiIiISBYYSoiIiEgWZLdLsBACAGAwGBxcEiIiIqqsku/tku/x6pBdKMnIyAAAhISEOLgkREREVFUZGRnQ6/XVulclahJpaoHRaMSVK1fg6ekJlUpl12cbDAaEhIQgMTERXl5edn22nLCeDYcS6giwng2NEuqphDoCVaunEAIZGRkIDg6GWl290SGyaylRq9Vo2rRprX6Gl5dXg/5LVIL1bDiUUEeA9WxolFBPJdQRqHw9q9tCUoIDXYmIiEgWGEqIiIhIFhQVSnQ6HV599VXodDpHF6VWsZ4NhxLqCLCeDY0S6qmEOgJ1X0/ZDXQlIiIiZVJUSwkRERHJF0MJERERyQJDCREREckCQwkRERHJAkMJERERyYKiQsknn3yCsLAwuLi4oGfPntizZ4+ji1RpCxYsQPfu3eHp6Ql/f3+MHTsWJ0+eNLsmJycH06ZNg6+vLzw8PHDXXXchOTnZ7JqLFy8iJiYGbm5u8Pf3x8yZM1FQUFCXVam0hQsXQqVSYfr06dKxhlLHy5cvY9KkSfD19YWrqys6duyIuLg46bwQAnPnzkVQUBBcXV0RHR2N06dPmz3j5s2bmDhxIry8vODt7Y2HH34YmZmZdV0VmwoLCzFnzhyEh4fD1dUVLVq0wPz5880266qP9dy2bRvuuOMOBAcHQ6VSYc2aNWbn7VWnw4cPo3///nBxcUFISAjeeeed2q6amfLqmZ+fj1mzZqFjx45wd3dHcHAwHnjgAVy5csXsGXKvZ0X/LU098cQTUKlUWLRokdlxudcRqFw9jx8/jtGjR0Ov18Pd3R3du3fHxYsXpfN19rtXKMT3338vnJ2dxRdffCGOHj0qHn30UeHt7S2Sk5MdXbRKGT58uPjyyy9FfHy8OHjwoLj99ttFaGioyMzMlK554oknREhIiNi4caOIi4sTvXr1En369JHOFxQUiA4dOojo6Ghx4MAB8eeffwo/Pz8xe/ZsR1SpXHv27BFhYWGiU6dO4tlnn5WON4Q63rx5UzRr1kw8+OCDYvfu3eLcuXNi3bp14syZM9I1CxcuFHq9XqxZs0YcOnRIjB49WoSHh4tbt25J14wYMUJERkaKXbt2ie3bt4uWLVuKCRMmOKJKVr355pvC19dX/P777yIhIUGsXLlSeHh4iA8//FC6pj7W888//xQvv/yyWLVqlQAgVq9ebXbeHnVKT08XAQEBYuLEiSI+Pl589913wtXVVfz3v/+tq2qWW8+0tDQRHR0tfvjhB3HixAkRGxsrevToIbp27Wr2DLnXs6L/liVWrVolIiMjRXBwsPjggw/Mzsm9jkJUXM8zZ84IHx8fMXPmTLF//35x5swZ8csvv5h9P9bV717FhJIePXqIadOmSe8LCwtFcHCwWLBggQNLVX0pKSkCgNi6dasQouiXhJOTk1i5cqV0zfHjxwUAERsbK4Qo+oupVqtFUlKSdM3ixYuFl5eXyM3NrdsKlCMjI0O0atVKrF+/XgwcOFAKJQ2ljrNmzRL9+vWzed5oNIrAwEDx73//WzqWlpYmdDqd+O6774QQQhw7dkwAEHv37pWu+euvv4RKpRKXL1+uvcJXQUxMjJg6darZsXHjxomJEycKIRpGPcv+grdXnT799FPRqFEjs7+zs2bNEm3atKnlGllX3hd2iT179ggA4sKFC0KI+ldPW3W8dOmSaNKkiYiPjxfNmjUzCyX1rY5CWK/nfffdJyZNmmTznrr83auI7pu8vDzs27cP0dHR0jG1Wo3o6GjExsY6sGTVl56eDgDw8fEBAOzbtw/5+flmdYyIiEBoaKhUx9jYWHTs2BEBAQHSNcOHD4fBYMDRo0frsPTlmzZtGmJiYszqAjScOv7666/o1q0b7rnnHvj7+yMqKgr/+9//pPMJCQlISkoyq6der0fPnj3N6unt7Y1u3bpJ10RHR0OtVmP37t11V5ly9OnTBxs3bsSpU6cAAIcOHcKOHTswcuRIAA2nnqbsVafY2FgMGDAAzs7O0jXDhw/HyZMnkZqaWke1qZr09HSoVCp4e3sDaBj1NBqNmDx5MmbOnIn27dtbnG8odfzjjz/QunVrDB8+HP7+/ujZs6dZF09d/u5VRCi5fv06CgsLzf6wACAgIABJSUkOKlX1GY1GTJ8+HX379kWHDh0AAElJSXB2dpZ+IZQwrWNSUpLVP4OSc3Lw/fffY//+/ViwYIHFuYZSx3PnzmHx4sVo1aoV1q1bhyeffBLPPPMMvvrqKwCl5Szv72tSUhL8/f3Nzmu1Wvj4+Mimni+99BLGjx+PiIgIODk5ISoqCtOnT8fEiRMBNJx6mrJXnerD32NTOTk5mDVrFiZMmCDtJNsQ6vn2229Dq9XimWeesXq+IdQxJSUFmZmZWLhwIUaMGIG///4bd955J8aNG4etW7cCqNvfvdoa1IUcZNq0aYiPj8eOHTscXRS7SkxMxLPPPov169fDxcXF0cWpNUajEd26dcNbb70FAIiKikJ8fDyWLFmCKVOmOLh09vPjjz/i22+/xYoVK9C+fXscPHgQ06dPR3BwcIOqp9Ll5+fj3nvvhRACixcvdnRx7Gbfvn348MMPsX//fqhUKkcXp9YYjUYAwJgxY/Dcc88BADp37oydO3diyZIlGDhwYJ2WRxEtJX5+ftBoNBYjhZOTkxEYGOigUlXP008/jd9//x2bN29G06ZNpeOBgYHIy8tDWlqa2fWmdQwMDLT6Z1ByztH27duHlJQUdOnSBVqtFlqtFlu3bsVHH30ErVaLgICAel9HAAgKCkK7du3MjrVt21Ya6V5SzvL+vgYGBiIlJcXsfEFBAW7evCmbes6cOVNqLenYsSMmT56M5557TmoFayj1NGWvOtWHv8dAaSC5cOEC1q9fL7WSAPW/ntu3b0dKSgpCQ0Ol30cXLlzAjBkzEBYWJpWxPtcRKPp+1Gq1Ff5OqqvfvYoIJc7OzujatSs2btwoHTMajdi4cSN69+7twJJVnhACTz/9NFavXo1NmzYhPDzc7HzXrl3h5ORkVseTJ0/i4sWLUh179+6NI0eOmP1PVPKLpOxfSEcYMmQIjhw5goMHD0o/3bp1w8SJE6XX9b2OANC3b1+L6dynTp1Cs2bNAADh4eEIDAw0q6fBYMDu3bvN6pmWloZ9+/ZJ12zatAlGoxE9e/asg1pULDs7G2q1+a8YjUYj/cusodTTlL3q1Lt3b2zbtg35+fnSNevXr0ebNm3QqFGjOqpN+UoCyenTp7Fhwwb4+vqana/v9Zw8eTIOHz5s9vsoODgYM2fOxLp16wDU/zoCRd+P3bt3L/d3Up1+v1R6SGw99/333wudTieWLVsmjh07Jh577DHh7e1tNlJYzp588kmh1+vFli1bxNWrV6Wf7Oxs6ZonnnhChIaGik2bNom4uDjRu3dv0bt3b+l8yZStYcOGiYMHD4q1a9eKxo0by2q6bFmms2+EaBh13LNnj9BqteLNN98Up0+fFt9++61wc3MT33zzjXTNwoULhbe3t/jll1/E4cOHxZgxY6xOK42KihK7d+8WO3bsEK1atZLVlOApU6aIJk2aSFOCV61aJfz8/MSLL74oXVMf65mRkSEOHDggDhw4IACI999/Xxw4cECadWKPOqWlpYmAgAAxefJkER8fL77//nvh5uZWp9NIy6tnXl6eGD16tGjatKk4ePCg2e8k05kWcq9nRf8tyyo7+0YI+ddRiIrruWrVKuHk5CSWLl0qTp8+LT7++GOh0WjE9u3bpWfU1e9exYQSIYT4+OOPRWhoqHB2dhY9evQQu3btcnSRKg2A1Z8vv/xSuubWrVviqaeeEo0aNRJubm7izjvvFFevXjV7zvnz58XIkSOFq6ur8PPzEzNmzBD5+fl1XJvKKxtKGkodf/vtN9GhQweh0+lERESEWLp0qdl5o9Eo5syZIwICAoROpxNDhgwRJ0+eNLvmxo0bYsKECcLDw0N4eXmJhx56SGRkZNRlNcplMBjEs88+K0JDQ4WLi4to3ry5ePnll82+tOpjPTdv3mz1/8UpU6YIIexXp0OHDol+/foJnU4nmjRpIhYuXFhXVRRClF/PhIQEm7+TNm/eXG/qWdF/y7KshRK511GIytXz888/Fy1bthQuLi4iMjJSrFmzxuwZdfW7VyWEyfKKRERERA6iiDElREREJH8MJURERCQLDCVEREQkCwwlREREJAsMJURERCQLDCVEREQkCwwlREREJAsMJURERCQLDCVEREQkCwwlREREJAsMJURERCQL/w8YvqiupbDflgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids for woman is [24626]\n",
      "Top 25 closest tokens to woman by cosine similarity: \n",
      "Token: 25277,     progressed,   Similarity: 0.1799\n",
      "Token: 37415,          Aires,   Similarity: 0.1737\n",
      "Token: 43839,      emanating,   Similarity: 0.1735\n",
      "Token: 32466,       suitable,   Similarity: 0.1658\n",
      "Token: 28719,      incapable,   Similarity: 0.1655\n",
      "Token: 18918,           hadn,   Similarity: 0.1607\n",
      "Token: 10606,        shouldn,   Similarity: 0.1602\n",
      "Token: 21741,         ostics,   Similarity: 0.1592\n",
      "Token: 12829,      succeeded,   Similarity: 0.1588\n",
      "Token: 15979,       stumbled,   Similarity: 0.1580\n",
      "Token: 41425,       armament,   Similarity: 0.1580\n",
      "Token: 35771,           Hume,   Similarity: 0.1573\n",
      "Token: 47421,          unals,   Similarity: 0.1559\n",
      "Token: 38298,          Roads,   Similarity: 0.1557\n",
      "Token: 11535,          weren,   Similarity: 0.1556\n",
      "Token: 41894,         skills,   Similarity: 0.1522\n",
      "Token: 38213,          isins,   Similarity: 0.1520\n",
      "Token:  9988,         itches,   Similarity: 0.1519\n",
      "Token:  7556,       supposed,   Similarity: 0.1517\n",
      "Token: 34389,          Poles,   Similarity: 0.1507\n",
      "Token: 13204,         nesday,   Similarity: 0.1502\n",
      "Token:  3719,          efore,   Similarity: 0.1501\n",
      "Token:  6134,          chers,   Similarity: 0.1500\n",
      "Token: 48641,     mattresses,   Similarity: 0.1496\n",
      "Token: 29399,     institutes,   Similarity: 0.1495\n"
     ]
    }
   ],
   "source": [
    "# Assuming the embedding layer is already trained\n",
    "layer = model.embedding.to('cpu')\n",
    "\n",
    "# Get the embedding for 'king' (e.g., assuming 'king' corresponds to token index 0)\n",
    "k = 200\n",
    "word = 'woman'\n",
    "\n",
    "print (f\"Input ids for {word} is {tokenizer(word)['input_ids']}\")\n",
    "\n",
    "word_index = tokenizer(word)['input_ids'][0]\n",
    "word_embedding = layer(torch.tensor(word_index)).detach()  # Get the embedding for 'king'\n",
    "\n",
    "# Get all embeddings\n",
    "all_embeddings = layer.weight.detach()  # Extract all embeddings\n",
    "\n",
    "\n",
    "# Compute cosine similarity between 'king' and all other tokens\n",
    "cosine_similarities = F.cosine_similarity(word_embedding.unsqueeze(0), all_embeddings, dim=1)\n",
    "\n",
    "# Get the indices of the 10 most similar tokens (excluding 'king' itself)\n",
    "top_k = torch.topk(cosine_similarities, k=k+1)  # Top 11 because 'king' will be in the list\n",
    "top_k_indices = top_k.indices[top_k.indices != word_index][:k]  # Exclude 'king'\n",
    "\n",
    "\n",
    "# Print the 10 closest tokens and their similarities\n",
    "print(f\"Top 25 closest tokens to {word} by cosine similarity: \")\n",
    "ii = 0\n",
    "for idx in top_k_indices:\n",
    "    #print(idx.item())\n",
    "    idx_word = tokenizer.decode(idx.item()).replace('\\n','')\n",
    "    if len(idx_word) > 4:\n",
    "        print(f\"Token: {idx.item():5d}, {idx_word:>14s},   Similarity: {cosine_similarities[idx].item():.4f}\")\n",
    "        ii += 1\n",
    "        if ii > 24:\n",
    "            break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 25 closest tokens to woman by distance: \n",
      "Token: 42765,      vibrating,   Distance: 0.77\n",
      "Token:  6134,          chers,   Distance: 0.78\n",
      "Token: 38298,          Roads,   Distance: 0.78\n",
      "Token: 21741,         ostics,   Distance: 0.78\n",
      "Token: 37415,          Aires,   Distance: 0.78\n",
      "Token: 47941,       Nagasaki,   Distance: 0.78\n",
      "Token:  6209,         cience,   Distance: 0.78\n",
      "Token: 34389,          Poles,   Distance: 0.78\n",
      "Token: 35771,           Hume,   Distance: 0.78\n",
      "Token:  8980,         Centre,   Distance: 0.78\n",
      "Token:  8739,           rose,   Distance: 0.78\n",
      "Token: 47669,           Tags,   Distance: 0.78\n",
      "Token: 44621,         pylori,   Distance: 0.78\n",
      "Token: 49019,          henes,   Distance: 0.78\n",
      "Token: 13281,       pipeline,   Distance: 0.78\n",
      "Token: 31326,          pedal,   Distance: 0.78\n",
      "Token: 29140,           Owen,   Distance: 0.78\n",
      "Token: 40496,         canola,   Distance: 0.79\n",
      "Token: 22534,          uncle,   Distance: 0.79\n",
      "Token: 43443,         Groups,   Distance: 0.79\n",
      "Token: 35600,         rabble,   Distance: 0.79\n",
      "Token: 36190,         sprout,   Distance: 0.79\n",
      "Token: 21821,           calf,   Distance: 0.79\n",
      "Token: 23843,      carbonate,   Distance: 0.79\n",
      "Token: 21111,           kiss,   Distance: 0.79\n"
     ]
    }
   ],
   "source": [
    "distances = torch.norm(all_embeddings - word_embedding, dim=1)  # Compute L2 norm (Euclidean distance)\n",
    "\n",
    "# Get the indices of the 10 closest tokens (excluding 'king' itself)\n",
    "top_k = torch.topk(-distances, k=k+1)  # Use negative distances to mimic \\\"closest\\\"\n",
    "top_k_indices = top_k.indices[top_k.indices != word_index][:k]  # Exclude 'king'\n",
    "\n",
    "# Print the 10 closest tokens and their distances\n",
    "print(f\"\\nTop 25 closest tokens to {word} by distance: \")\n",
    "ii = 0\n",
    "for idx in top_k_indices:\n",
    "    #print(idx.item())\n",
    "    idx_word = tokenizer.decode(idx.item()).replace('\\n','')\n",
    "    if len(idx_word) > 4:    \n",
    "        print(f\"Token: {idx.item():5d}, {tokenizer.decode(idx.item()).replace('\\n',''):>14s},   Distance: {distances[idx].item():.2f}\")\n",
    "        ii += 1\n",
    "        if ii > 24:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.558848\n"
     ]
    }
   ],
   "source": [
    "# We will inherit from Pytorch's Module class\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, n_embed: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Map tokens to embedding space with dimension n_embed\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=n_embed)\n",
    "\n",
    "        self.fft1 = nn.Sequential(nn.Linear(n_embed, 3*n_embed),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(n_embed, 3*n_embed))\n",
    "        \n",
    "        self.fft2 = nn.Sequential(nn.Linear(n_embed, 3*n_embed),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(n_embed, 3*n_embed))\n",
    "                               \n",
    "\n",
    "        # Now map back to make prediction for next tokens, called logits.  The layer that\n",
    "        # does this is traditionally a linear layer called the lannguage model head.\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.embedding.weight = self.lm_head.weight\n",
    "        \n",
    "\n",
    "    # Pytorch Modules have a forward method that is what is executed when the model is called\n",
    "    # The input is the (batch, tokens) tensor and a (batch, logits) tensor is returned\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = self.fft1(x)\n",
    "        x = self.fft2(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "    \n",
    "# Let's create the model\n",
    "model = EmbeddingModel(vocab_size = tokenizer.vocab_size, n_embed=704)\n",
    "\n",
    "# Let's see how many parameters we have\n",
    "print (sum([p.numel() for p in model.parameters()])/1.0e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "40*7/5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
