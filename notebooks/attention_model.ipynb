{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a seed for reproduceability\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model: MLPs, Weight Tieing, and batch size\n",
    "\n",
    "In this notebook, we explore what happens if we turn the final lm_head output layer into a couple of MLP layers, keeping the parameter count around 50M. We also add some complexity to the training loop such as batch size and warmup steps.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Tokenizer\n",
    "\n",
    "# 512 is a good minimal context length in principle large enough to understand paragraphs\n",
    "context_length = 512\n",
    "\n",
    "# The original Llama 2 tokenizer is available from Huggingface, but we will use the more\n",
    "# up-to-date Cosmo2 toenizer provided by Huggingface for the Llama-like SmolLM models\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/cosmo2-tokenizer\")\n",
    "tokenizer.model_max_length = context_length\n",
    "# There are times we need to pad.  We will make the padding token the same one that \n",
    "# signals end of sentance.\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed33bceb00d47d4868c31f95dc76e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c3230655fd410c83ce3a45041bcd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set dataset\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Load dataset from hugging face in streaming mode.\n",
    "dataset = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True)\n",
    "\n",
    "# Create a tokenize fundtion.\n",
    "def tokenize(item):\n",
    "    x = tokenizer(\n",
    "        item['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset.column_names\n",
    ")\n",
    "train_dl = DataLoader(dataset=tokenized_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building linear model\n",
    "\n",
    "Now let's make a linear model using a couple of back to back MLPs with Relu activation functions to explore if this is superior to just the embedding layer with linear output, assuming parameters are fixed to roughly 50M parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, n_embed: int):\n",
    "        super().__init__()\n",
    "\n",
    "        assert n_embed % n_heads == 0, \"Number of heads muct divide embedding dimension\"\n",
    "        self.n_heads = n_heads\n",
    "        self.h_dim = n_embed // n_heads\n",
    "\n",
    "        self.k = nn.Linear(n_embed, n_embed)\n",
    "        self.q = nn.Linear(n_embed, n_embed)\n",
    "        self.v= nn.Linear(n_embed, n_embed)\n",
    "\n",
    "        self.proj_out= nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward (self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # (B, T, C) -> (B, T, n_heads, h_dim) -> (B, n_heads, T, h_dim)\n",
    "        q = self.q(x).view(B, T, self.n_heads, self.h_dim).transpose(1,2)\n",
    "        k = self.k(x).view(B, T, self.n_heads, self.h_dim).transpose(1,2)\n",
    "        v = self.v(x).view(B, T, self.n_heads, self.h_dim).transpose(1,2)\n",
    "\n",
    "        x = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        # (B, n_heads, T, h_dim) -> (B, T, n_heads, h_dim) -> (B, T, C)\n",
    "        x = x.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj_out(x)\n",
    "\n",
    "\n",
    "# We will inherit from Pytorch's Module class\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, n_embed: int, n_heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Map tokens to embedding space with dimension n_embed\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=n_embed)\n",
    "\n",
    "        self.attn = MultiHeadedSelfAttention(n_heads, n_embed)\n",
    "        \n",
    "        self.fft = nn.Sequential(nn.Linear(n_embed, 4*n_embed),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(4*n_embed, n_embed))\n",
    "                               \n",
    "\n",
    "        # Now map back to make prediction for next tokens, called logits.  The layer that\n",
    "        # does this is traditionally a linear layer called the lannguage model head.\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.embedding.weight = self.lm_head.weight\n",
    "        \n",
    "\n",
    "    # Pytorch Modules have a forward method that is what is executed when the model is called\n",
    "    # The input is the (batch, tokens) tensor and a (batch, logits) tensor is returned\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.fft(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.20864\n"
     ]
    }
   ],
   "source": [
    "# Let's create the model\n",
    "model = AttentionModel(vocab_size = tokenizer.vocab_size, n_embed=832, n_heads=13)\n",
    "\n",
    "# Let's see how many parameters we have\n",
    "print (sum([p.numel() for p in model.parameters()])/1.0e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "\n",
    "Let's add the batch size to be 250k tokens.  Like Smallest Llama models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3738443087.py, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 43\u001b[0;36m\u001b[0m\n\u001b[0;31m    if (acc_step - 1) %  == 0:\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\"\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "accum_iter = 128 // batch_size\n",
    "w_steps = 2000\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=w_steps)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters()]) / 1.0e6\n",
    "print(f\"This model has {num_params:.2f}M  params\")\n",
    "print(\n",
    "    f\"The vocab size is {tokenizer.vocab_size}, meaning the initial loss should be ~{math.log(tokenizer.vocab_size):.3f}\"\n",
    ")\n",
    "model.train()\n",
    "step_losses = []\n",
    "losses = []\n",
    "for step, batch in enumerate(train_dl):\n",
    "\n",
    "    # Get data\n",
    "    x = batch[\"input_ids\"].to(device)\n",
    "    att_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Compute prediction error\n",
    "    logits = model(x)\n",
    "\n",
    "    # Shift to compare loss correctly\n",
    "    shifted_logits = logits[:, :-1].contiguous()\n",
    "    labels = x[:, 1:].contiguous()\n",
    "    loss = loss_fn(shifted_logits.view(-1, shifted_logits.size(-1)), labels.view(-1))\n",
    "    loss = loss / accum_iter\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    step_losses.append(loss.item())\n",
    "\n",
    "    if (step + 1) % accum_iter == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(sum(step_losses[-accum_iter:]))\n",
    "\n",
    "        acc_step = (step + 1) // accum_iter\n",
    "        scheduler.step()\n",
    "        if (acc_step - 1) % 5 == 0:\n",
    "            print(\n",
    "                f\"Step: {acc_step:>5d},  Tokens:{batch_size*context_length*accum_iter*acc_step/1.0e6:>7.3f}M,  LR: {scheduler.get_last_lr()[-1]:.2e}, Loss: {losses[-1]:>7.3f}, Perplexity:  {math.exp(losses[-1]):>7.1f}\"\n",
    "            )\n",
    "        \n",
    "    if (step + 1) % 12500 == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1509c2660>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids for woman is [24626]\n",
      "Top 25 closest tokens to woman by cosine similarity: \n",
      "Token: 26579,         Volume,   Similarity: 0.1624\n",
      "Token: 20421,      ategories,   Similarity: 0.1595\n",
      "Token: 24254,          Pluto,   Similarity: 0.1593\n",
      "Token: 45438,       executes,   Similarity: 0.1563\n",
      "Token: 43342,          erary,   Similarity: 0.1530\n",
      "Token:  9884,          ceans,   Similarity: 0.1521\n",
      "Token: 20294,        inhibit,   Similarity: 0.1515\n",
      "Token: 26732,        onsored,   Similarity: 0.1512\n",
      "Token: 22907,          Griff,   Similarity: 0.1499\n",
      "Token: 45544,      histogram,   Similarity: 0.1498\n",
      "Token: 26303,        sinking,   Similarity: 0.1492\n",
      "Token: 18873,       responds,   Similarity: 0.1473\n",
      "Token: 37416,         thresh,   Similarity: 0.1463\n",
      "Token: 39462,       Proteins,   Similarity: 0.1457\n",
      "Token: 41249,   Surprisingly,   Similarity: 0.1453\n",
      "Token: 34142,         sheath,   Similarity: 0.1436\n",
      "Token: 48691,        alerted,   Similarity: 0.1427\n",
      "Token:  7307,          fract,   Similarity: 0.1410\n",
      "Token: 15560,          embod,   Similarity: 0.1410\n",
      "Token: 13798,          Tibet,   Similarity: 0.1401\n",
      "Token: 24702,        Sabbath,   Similarity: 0.1395\n",
      "Token: 46994,       hydrogen,   Similarity: 0.1391\n",
      "Token: 28042,     extracting,   Similarity: 0.1383\n",
      "Token:  6845,      simultane,   Similarity: 0.1379\n",
      "Token: 15000,       instinct,   Similarity: 0.1377\n"
     ]
    }
   ],
   "source": [
    "# Assuming the embedding layer is already trained\n",
    "layer = model.embedding.to('cpu')\n",
    "\n",
    "# Get the embedding for 'king' (e.g., assuming 'king' corresponds to token index 0)\n",
    "k = 200\n",
    "word = 'woman'\n",
    "\n",
    "print (f\"Input ids for {word} is {tokenizer(word)['input_ids']}\")\n",
    "\n",
    "word_index = tokenizer(word)['input_ids'][0]\n",
    "word_embedding = layer(torch.tensor(word_index)).detach()  # Get the embedding for 'king'\n",
    "\n",
    "# Get all embeddings\n",
    "all_embeddings = layer.weight.detach()  # Extract all embeddings\n",
    "\n",
    "\n",
    "# Compute cosine similarity between 'king' and all other tokens\n",
    "cosine_similarities = F.cosine_similarity(word_embedding.unsqueeze(0), all_embeddings, dim=1)\n",
    "\n",
    "# Get the indices of the 10 most similar tokens (excluding 'king' itself)\n",
    "top_k = torch.topk(cosine_similarities, k=k+1)  # Top 11 because 'king' will be in the list\n",
    "top_k_indices = top_k.indices[top_k.indices != word_index][:k]  # Exclude 'king'\n",
    "\n",
    "\n",
    "# Print the 10 closest tokens and their similarities\n",
    "print(f\"Top 25 closest tokens to {word} by cosine similarity: \")\n",
    "ii = 0\n",
    "for idx in top_k_indices:\n",
    "    #print(idx.item())\n",
    "    idx_word = tokenizer.decode(idx.item()).replace('\\n','')\n",
    "    if len(idx_word) > 4:\n",
    "        print(f\"Token: {idx.item():5d}, {idx_word:>14s},   Similarity: {cosine_similarities[idx].item():.4f}\")\n",
    "        ii += 1\n",
    "        if ii > 24:\n",
    "            break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 25 closest tokens to woman by distance: \n",
      "Token: 43342,          erary,   Distance: 0.74\n",
      "Token: 30409,     externally,   Distance: 0.75\n",
      "Token: 24254,          Pluto,   Distance: 0.75\n",
      "Token: 45438,       executes,   Distance: 0.75\n",
      "Token:  7307,          fract,   Distance: 0.75\n",
      "Token: 24702,        Sabbath,   Distance: 0.75\n",
      "Token: 18873,       responds,   Distance: 0.75\n",
      "Token: 41249,   Surprisingly,   Distance: 0.75\n",
      "Token: 48174,      motorized,   Distance: 0.75\n",
      "Token: 20421,      ategories,   Distance: 0.75\n",
      "Token:  6125,        Pacific,   Distance: 0.76\n",
      "Token: 22907,          Griff,   Distance: 0.76\n",
      "Token: 33382,        Removal,   Distance: 0.76\n",
      "Token:  9202,          royal,   Distance: 0.76\n",
      "Token: 26579,         Volume,   Distance: 0.76\n",
      "Token: 39462,       Proteins,   Distance: 0.76\n",
      "Token: 36216,        thirsty,   Distance: 0.76\n",
      "Token: 28331,          guild,   Distance: 0.76\n",
      "Token: 15560,          embod,   Distance: 0.76\n",
      "Token: 38793,      degrading,   Distance: 0.76\n",
      "Token: 44462,         straps,   Distance: 0.76\n",
      "Token: 20294,        inhibit,   Distance: 0.76\n",
      "Token: 19947,     Elementary,   Distance: 0.76\n",
      "Token: 25746,         sender,   Distance: 0.76\n",
      "Token: 13798,          Tibet,   Distance: 0.76\n"
     ]
    }
   ],
   "source": [
    "distances = torch.norm(all_embeddings - word_embedding, dim=1)  # Compute L2 norm (Euclidean distance)\n",
    "\n",
    "# Get the indices of the 10 closest tokens (excluding 'king' itself)\n",
    "top_k = torch.topk(-distances, k=k+1)  # Use negative distances to mimic \\\"closest\\\"\n",
    "top_k_indices = top_k.indices[top_k.indices != word_index][:k]  # Exclude 'king'\n",
    "\n",
    "# Print the 10 closest tokens and their distances\n",
    "print(f\"\\nTop 25 closest tokens to {word} by distance: \")\n",
    "ii = 0\n",
    "for idx in top_k_indices:\n",
    "    #print(idx.item())\n",
    "    idx_word = tokenizer.decode(idx.item()).replace('\\n','')\n",
    "    if len(idx_word) > 4:    \n",
    "        print(f\"Token: {idx.item():5d}, {tokenizer.decode(idx.item()).replace('\\n',''):>14s},   Distance: {distances[idx].item():.2f}\")\n",
    "        ii += 1\n",
    "        if ii > 24:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.558848\n"
     ]
    }
   ],
   "source": [
    "# We will inherit from Pytorch's Module class\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, n_embed: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Map tokens to embedding space with dimension n_embed\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=n_embed)\n",
    "\n",
    "        self.fft1 = nn.Sequential(nn.Linear(n_embed, 3*n_embed),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(n_embed, 3*n_embed))\n",
    "        \n",
    "        self.fft2 = nn.Sequential(nn.Linear(n_embed, 3*n_embed),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(n_embed, 3*n_embed))\n",
    "                               \n",
    "\n",
    "        # Now map back to make prediction for next tokens, called logits.  The layer that\n",
    "        # does this is traditionally a linear layer called the lannguage model head.\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.embedding.weight = self.lm_head.weight\n",
    "        \n",
    "\n",
    "    # Pytorch Modules have a forward method that is what is executed when the model is called\n",
    "    # The input is the (batch, tokens) tensor and a (batch, logits) tensor is returned\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = self.fft1(x)\n",
    "        x = self.fft2(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "    \n",
    "# Let's create the model\n",
    "model = EmbeddingModel(vocab_size = tokenizer.vocab_size, n_embed=704)\n",
    "\n",
    "# Let's see how many parameters we have\n",
    "print (sum([p.numel() for p in model.parameters()])/1.0e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
